{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Reads\n",
    "\n",
    "Goodreads Books Review Rating Prediction\n",
    "Reviews are a good way to judge the quality of any product, whether it's books, clothes, technology, or anything else. When you want to buy something online these days, the first thing that comes to mind is the reviews from past buyers and the overall rating the product has received.\n",
    "Reader feedback, whether positive or negative, five stars or one star, will encourage the product owner to make improvements.\n",
    "Reader connection and engagement will be encouraged by book reviews, whether they be left on Amazon, Goodreads, or social media. Readers must determine whether or not other readers are enjoying the book.\n",
    "\n",
    "In this competition you will work with a challenging dataset consisting reviews from the Goodreads book review website, and a variety of attributes describing the items. and you have to predict review rating which ranges from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk\n",
    "#pip install spacy\n",
    "# pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from textblob import TextBlob #sentiment anlysis library\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer #another sentiment analysis library\n",
    "#import spacy #another sentiment analysis library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #the following 4 libraries are for a customize model of sentiment with sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "from keras.preprocessing.text import Tokenizer #the following 4 libraries are for a customize model of sentiment with deep learning: keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf # This library is for a customize model of sentiement with deep learning: Tensor flow\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900000, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>18245960</td>\n",
       "      <td>dfdbb7b0eb5a7e4c26d59a937e2e5feb</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a special book. It started slow for ab...</td>\n",
       "      <td>Sun Jul 30 07:44:10 -0700 2017</td>\n",
       "      <td>Wed Aug 30 00:00:26 -0700 2017</td>\n",
       "      <td>Sat Aug 26 12:05:52 -0700 2017</td>\n",
       "      <td>Tue Aug 15 13:23:18 -0700 2017</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>16981</td>\n",
       "      <td>a5d2c3628987712d0e05c4f90798eb67</td>\n",
       "      <td>3</td>\n",
       "      <td>Recommended by Don Katz. Avail for free in Dec...</td>\n",
       "      <td>Mon Dec 05 10:46:44 -0800 2016</td>\n",
       "      <td>Wed Mar 22 11:37:04 -0700 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>28684704</td>\n",
       "      <td>2ede853b14dc4583f96cf5d120af636f</td>\n",
       "      <td>3</td>\n",
       "      <td>A fun, fast paced science fiction thriller. I ...</td>\n",
       "      <td>Tue Nov 15 11:29:22 -0800 2016</td>\n",
       "      <td>Mon Mar 20 23:40:27 -0700 2017</td>\n",
       "      <td>Sat Mar 18 23:22:42 -0700 2017</td>\n",
       "      <td>Fri Mar 17 23:45:40 -0700 2017</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>27161156</td>\n",
       "      <td>ced5675e55cd9d38a524743f5c40996e</td>\n",
       "      <td>0</td>\n",
       "      <td>Recommended reading to understand what is goin...</td>\n",
       "      <td>Wed Nov 09 17:37:04 -0800 2016</td>\n",
       "      <td>Wed Nov 09 17:38:20 -0800 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>25884323</td>\n",
       "      <td>332732725863131279a8e345b63ac33e</td>\n",
       "      <td>4</td>\n",
       "      <td>I really enjoyed this book, and there is a lot...</td>\n",
       "      <td>Mon Apr 25 09:31:23 -0700 2016</td>\n",
       "      <td>Mon Apr 25 09:31:23 -0700 2016</td>\n",
       "      <td>Sun Jun 26 00:00:00 -0700 2016</td>\n",
       "      <td>Sat May 28 00:00:00 -0700 2016</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id   book_id  \\\n",
       "0  8842281e1d1347389f2ab93d60773d4d  18245960   \n",
       "1  8842281e1d1347389f2ab93d60773d4d     16981   \n",
       "2  8842281e1d1347389f2ab93d60773d4d  28684704   \n",
       "3  8842281e1d1347389f2ab93d60773d4d  27161156   \n",
       "4  8842281e1d1347389f2ab93d60773d4d  25884323   \n",
       "\n",
       "                          review_id  rating  \\\n",
       "0  dfdbb7b0eb5a7e4c26d59a937e2e5feb       5   \n",
       "1  a5d2c3628987712d0e05c4f90798eb67       3   \n",
       "2  2ede853b14dc4583f96cf5d120af636f       3   \n",
       "3  ced5675e55cd9d38a524743f5c40996e       0   \n",
       "4  332732725863131279a8e345b63ac33e       4   \n",
       "\n",
       "                                         review_text  \\\n",
       "0  This is a special book. It started slow for ab...   \n",
       "1  Recommended by Don Katz. Avail for free in Dec...   \n",
       "2  A fun, fast paced science fiction thriller. I ...   \n",
       "3  Recommended reading to understand what is goin...   \n",
       "4  I really enjoyed this book, and there is a lot...   \n",
       "\n",
       "                       date_added                    date_updated  \\\n",
       "0  Sun Jul 30 07:44:10 -0700 2017  Wed Aug 30 00:00:26 -0700 2017   \n",
       "1  Mon Dec 05 10:46:44 -0800 2016  Wed Mar 22 11:37:04 -0700 2017   \n",
       "2  Tue Nov 15 11:29:22 -0800 2016  Mon Mar 20 23:40:27 -0700 2017   \n",
       "3  Wed Nov 09 17:37:04 -0800 2016  Wed Nov 09 17:38:20 -0800 2016   \n",
       "4  Mon Apr 25 09:31:23 -0700 2016  Mon Apr 25 09:31:23 -0700 2016   \n",
       "\n",
       "                          read_at                      started_at  n_votes  \\\n",
       "0  Sat Aug 26 12:05:52 -0700 2017  Tue Aug 15 13:23:18 -0700 2017       28   \n",
       "1                             NaN                             NaN        1   \n",
       "2  Sat Mar 18 23:22:42 -0700 2017  Fri Mar 17 23:45:40 -0700 2017       22   \n",
       "3                             NaN                             NaN        5   \n",
       "4  Sun Jun 26 00:00:00 -0700 2016  Sat May 28 00:00:00 -0700 2016        9   \n",
       "\n",
       "   n_comments  \n",
       "0           1  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "path = \".\\goodreads-books-reviews-290312\\goodreads_train.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop([\"Unnamed: 0\"], axis=1) #this just when I upload the sample of 10% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         object\n",
       "book_id          int64\n",
       "review_id       object\n",
       "rating           int64\n",
       "review_text     object\n",
       "date_added      object\n",
       "date_updated    object\n",
       "read_at         object\n",
       "started_at      object\n",
       "n_votes          int64\n",
       "n_comments       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check type var\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         900000\n",
       "book_id         900000\n",
       "review_id       900000\n",
       "rating          900000\n",
       "review_text     900000\n",
       "date_added      900000\n",
       "date_updated    900000\n",
       "read_at         808234\n",
       "started_at      625703\n",
       "n_votes         900000\n",
       "n_comments      900000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id              0\n",
       "book_id              0\n",
       "review_id            0\n",
       "rating               0\n",
       "review_text          0\n",
       "date_added           0\n",
       "date_updated         0\n",
       "read_at          91766\n",
       "started_at      274297\n",
       "n_votes              0\n",
       "n_comments           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>615507</td>\n",
       "      <td>6.155070e+05</td>\n",
       "      <td>615507</td>\n",
       "      <td>615507.000000</td>\n",
       "      <td>615507</td>\n",
       "      <td>615507</td>\n",
       "      <td>615507</td>\n",
       "      <td>615507</td>\n",
       "      <td>615507</td>\n",
       "      <td>615507.000000</td>\n",
       "      <td>615507.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>11610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>615507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>609939</td>\n",
       "      <td>613502</td>\n",
       "      <td>605825</td>\n",
       "      <td>261654</td>\n",
       "      <td>37628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>843a44e2499ba9362b47a089b0b0ce75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dfdbb7b0eb5a7e4c26d59a937e2e5feb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Review to come.</td>\n",
       "      <td>Wed Jul 14 21:11:00 -0700 2010</td>\n",
       "      <td>Mon Sep 23 05:22:17 -0700 2013</td>\n",
       "      <td>Fri Jan 01 00:00:00 -0800 2016</td>\n",
       "      <td>Fri Jan 01 00:00:00 -0800 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>440</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>432</td>\n",
       "      <td>827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.454812e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.800428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.716936</td>\n",
       "      <td>1.152703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.177933e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.123173</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.148401</td>\n",
       "      <td>6.106695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.735333e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.574919e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.185362e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.632868e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3167.000000</td>\n",
       "      <td>833.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 user_id       book_id  \\\n",
       "count                             615507  6.155070e+05   \n",
       "unique                             11610           NaN   \n",
       "top     843a44e2499ba9362b47a089b0b0ce75           NaN   \n",
       "freq                                1506           NaN   \n",
       "mean                                 NaN  1.454812e+07   \n",
       "std                                  NaN  9.177933e+06   \n",
       "min                                  NaN  1.000000e+00   \n",
       "25%                                  NaN  7.735333e+06   \n",
       "50%                                  NaN  1.574919e+07   \n",
       "75%                                  NaN  2.185362e+07   \n",
       "max                                  NaN  3.632868e+07   \n",
       "\n",
       "                               review_id         rating      review_text  \\\n",
       "count                             615507  615507.000000           615507   \n",
       "unique                            615507            NaN           609939   \n",
       "top     dfdbb7b0eb5a7e4c26d59a937e2e5feb            NaN  Review to come.   \n",
       "freq                                   1            NaN              440   \n",
       "mean                                 NaN       3.800428              NaN   \n",
       "std                                  NaN       1.123173              NaN   \n",
       "min                                  NaN       0.000000              NaN   \n",
       "25%                                  NaN       3.000000              NaN   \n",
       "50%                                  NaN       4.000000              NaN   \n",
       "75%                                  NaN       5.000000              NaN   \n",
       "max                                  NaN       5.000000              NaN   \n",
       "\n",
       "                            date_added                    date_updated  \\\n",
       "count                           615507                          615507   \n",
       "unique                          613502                          605825   \n",
       "top     Wed Jul 14 21:11:00 -0700 2010  Mon Sep 23 05:22:17 -0700 2013   \n",
       "freq                                13                              27   \n",
       "mean                               NaN                             NaN   \n",
       "std                                NaN                             NaN   \n",
       "min                                NaN                             NaN   \n",
       "25%                                NaN                             NaN   \n",
       "50%                                NaN                             NaN   \n",
       "75%                                NaN                             NaN   \n",
       "max                                NaN                             NaN   \n",
       "\n",
       "                               read_at                      started_at  \\\n",
       "count                           615507                          615507   \n",
       "unique                          261654                           37628   \n",
       "top     Fri Jan 01 00:00:00 -0800 2016  Fri Jan 01 00:00:00 -0800 2016   \n",
       "freq                               432                             827   \n",
       "mean                               NaN                             NaN   \n",
       "std                                NaN                             NaN   \n",
       "min                                NaN                             NaN   \n",
       "25%                                NaN                             NaN   \n",
       "50%                                NaN                             NaN   \n",
       "75%                                NaN                             NaN   \n",
       "max                                NaN                             NaN   \n",
       "\n",
       "              n_votes     n_comments  \n",
       "count   615507.000000  615507.000000  \n",
       "unique            NaN            NaN  \n",
       "top               NaN            NaN  \n",
       "freq              NaN            NaN  \n",
       "mean         3.716936       1.152703  \n",
       "std         18.148401       6.106695  \n",
       "min         -3.000000      -1.000000  \n",
       "25%          0.000000       0.000000  \n",
       "50%          0.000000       0.000000  \n",
       "75%          2.000000       0.000000  \n",
       "max       3167.000000     833.000000  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis 1: Textblob <br>\n",
    "In this code, we use the pandas library to read the book review data into a dataframe, and the TextBlob library to perform sentiment analysis. We apply the sentiment analysis to each review in the \"review\" column of the dataframe and store the results in a new column with the sentiment called 'review_text_textblob'. The polarity score of each review ranges from -1 (most negative) to 1 (most positive). <br>\n",
    "if you're using TextBlob for sentiment analysis, the model might use a pre-trained classifier or a dataset of annotated texts to determine the sentiment of a given review text. In this case, the training data for the sentiment analysis classifier could come from a variety of sources, such as movie reviews, product reviews, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edofa\\AppData\\Local\\Temp\\ipykernel_24444\\2307057653.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_text_textblob'] = df['review_text'].apply(lambda x: TextBlob(x).sentiment.polarity) #it takes 14 minutes with the completed data\n"
     ]
    }
   ],
   "source": [
    "# Create a new column in the dataframe to store the sentiment scores\n",
    "df['review_text_textblob'] = df['review_text'].apply(lambda x: TextBlob(x).sentiment.polarity) #it takes 14 minutes with the completed data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis 2: Nltk <br>\n",
    "In this code, we use the pandas library to read the book review data into a dataframe, and the SentimentIntensityAnalyzer from the nltk library to perform sentiment analysis. We apply the sentiment analysis to each review in the \"review\" column of the dataframe and store the results in a new column with the sentiment called 'review_text_nltk'. The polarity score of each review ranges from -1 (most negative) to 1 (most positive). <br>\n",
    " NLTK does provide some pretrained models, it's not a single pretrained model, but rather a collection of models, algorithms, and resources that can be used for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\edofa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SentimentIntensityAnalyzer\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edofa\\AppData\\Local\\Temp\\ipykernel_24444\\2154696478.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_text_nltk'] = df['review_text'].apply(lambda x: sia.polarity_scores(x)['compound']) #it takes 2 min with the sample data\n"
     ]
    }
   ],
   "source": [
    "# Create a new column in the dataframe to store the sentiment scores\n",
    "df['review_text_nltk'] = df['review_text'].apply(lambda x: sia.polarity_scores(x)['compound']) #it takes 17 min with the completed data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis 3: Spacy <br>\n",
    "In this code, we use the pandas library to read the book review data into a dataframe, and the spacy library to perform sentiment analysis. We apply the sentiment analysis to each review in the \"review\" column of the dataframe and store the results in a new column called \"sentiment\". The sentiment score of each review is either 1 (positive) or -1 (negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model\n",
    "#nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get the sentiment score of a review\n",
    "#def get_sentiment(review):\n",
    "    #doc = nlp(review)\n",
    "    #if doc.cats['POSITIVE'] > doc.cats['NEGATIVE']:\n",
    "        #return 1\n",
    "    #else:\n",
    "        #return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the dataframe to store the sentiment scores\n",
    "#df['review_text_spacy'] = df['review_text'].apply(lambda x: get_sentiment(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis 4: Customize model with sklearn <br>\n",
    "\n",
    "In this code, we use the pandas library to read the book review data into a dataframe and split it into training and testing sets.\n",
    "\n",
    "Next, we use the TfidfVectorizer from scikit-learn to convert the text data into numerical features using the Tf-idf representation.\n",
    "\n",
    "We then train a linear support vector machine (SVM) classifier using the LinearSVC class from scikit-learn on the training data.\n",
    "\n",
    "We use the trained model to make predictions on the test data and evaluate its accuracy using the accuracy_score function from scikit-learn.\n",
    "\n",
    "Finally, we add the prediction results as a new column in the dataframe using df.loc[] to assign the values only for the rows in the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review_text'], df['rating'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text data into numerical features using Tf-idf\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a linear support vector machine (SVM) classifier\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5238366016257521\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edofa\\AppData\\Local\\Temp\\ipykernel_24444\\2629232593.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_text_sktlearn'] = np.nan\n"
     ]
    }
   ],
   "source": [
    "# Add the prediction results as a new column in the dataframe\n",
    "df['review_text_sktlearn'] = np.nan\n",
    "df.loc[X_test.index, 'review_text_sktlearn'] = y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis 5: Keras <br>\n",
    "\n",
    "Here's a step-by-step explanation of the process for training a multi-class sentiment analysis model in Keras to predict the rating of a book review:\n",
    "\n",
    "Load the data: The first step is to load the book review data into a pandas dataframe. In this example, the data is loaded from a CSV file using the read_csv function.\n",
    "\n",
    "Preprocess the text data: The next step is to preprocess the text data. To do this, we use the Tokenizer class from Keras. We first fit the tokenizer on the text data, which creates a vocabulary of the most common words in the text. Then, we use the tokenizer to convert the text data into numerical sequences, where each word is represented by its index in the vocabulary. Finally, we use the pad_sequences function from Keras to pad the sequences to a fixed length of 100.\n",
    "\n",
    "Convert the rating data into categorical labels: We convert the rating data into categorical labels using the to_categorical function from Keras. This function converts the rating data into a one-hot encoded representation, where each rating is represented as a vector of 6 elements with a 1 in the index corresponding to the rating and 0s in all other elements.\n",
    "\n",
    "Split the data into training and testing sets: We split the preprocessed data into training and testing sets using the train_test_split function from scikit-learn. This function splits the data into two sets, one for training the model and one for evaluating its performance.\n",
    "\n",
    "Define the model architecture: We define the model architecture using the Sequential model from Keras. We add an Embedding layer, which maps the word indices to dense vectors of fixed size. The dense vectors are used as input to the next layer. We add a GRU (Gated Recurrent Unit) layer, which is a type of recurrent neural network that processes sequences of input data. The GRU layer allows the model to take into account the context and dependencies between words in the review. Finally, we add a Dense layer with a softmax activation function, which outputs the probability of each rating class.\n",
    "\n",
    "Compile the model: We compile the model using the compile method and specify the loss function, optimizer, and evaluation metrics. The loss function measures the difference between the predicted and actual ratings. The optimizer updates the model weights to minimize the loss. The evaluation metrics measure the performance of the model.\n",
    "\n",
    "Train the model: We train the model using the fit method. The model is trained on the training data, and the model weights are updated to minimize the loss function. The model is trained for 10 epochs, where an epoch is one pass over the training data. The batch size is set to 32, which means that the model updates its weights after processing 32 reviews at a time.\n",
    "\n",
    "Evaluate the model: Finally, we evaluate the model on the test data using the evaluate method. The evaluate method returns the loss and accuracy of the model on the test data.\n",
    "\n",
    "By following these steps, we can train a multi-class sentiment analysis model in Keras to predict the rating of a book review based on its text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['review_text'])\n",
    "X = tokenizer.texts_to_sequences(df['review_text'])\n",
    "X = pad_sequences(X, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the rating data into categorical labels\n",
    "y = to_categorical(df['rating'], num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 128, input_length=100))\n",
    "model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13465/13465 [==============================] - 1174s 87ms/step - loss: 1.0427 - accuracy: 0.5478 - val_loss: 0.9758 - val_accuracy: 0.5768\n",
      "Epoch 2/10\n",
      "13465/13465 [==============================] - 1203s 89ms/step - loss: 0.9558 - accuracy: 0.5874 - val_loss: 0.9550 - val_accuracy: 0.5883\n",
      "Epoch 3/10\n",
      "13465/13465 [==============================] - 1189s 88ms/step - loss: 0.9231 - accuracy: 0.6023 - val_loss: 0.9512 - val_accuracy: 0.5877\n",
      "Epoch 4/10\n",
      "13465/13465 [==============================] - 1196s 89ms/step - loss: 0.8997 - accuracy: 0.6130 - val_loss: 0.9482 - val_accuracy: 0.5879\n",
      "Epoch 5/10\n",
      "13465/13465 [==============================] - 1189s 88ms/step - loss: 0.8814 - accuracy: 0.6212 - val_loss: 0.9482 - val_accuracy: 0.5904\n",
      "Epoch 6/10\n",
      "13465/13465 [==============================] - 1216s 90ms/step - loss: 0.8680 - accuracy: 0.6284 - val_loss: 0.9540 - val_accuracy: 0.5895\n",
      "Epoch 7/10\n",
      "13465/13465 [==============================] - 1215s 90ms/step - loss: 0.8563 - accuracy: 0.6341 - val_loss: 0.9614 - val_accuracy: 0.5881\n",
      "Epoch 8/10\n",
      "13465/13465 [==============================] - 1211s 90ms/step - loss: 0.8462 - accuracy: 0.6383 - val_loss: 0.9686 - val_accuracy: 0.5858\n",
      "Epoch 9/10\n",
      "13465/13465 [==============================] - 1226s 91ms/step - loss: 0.8377 - accuracy: 0.6424 - val_loss: 0.9760 - val_accuracy: 0.5814\n",
      "Epoch 10/10\n",
      "13465/13465 [==============================] - 1228s 91ms/step - loss: 0.8310 - accuracy: 0.6461 - val_loss: 0.9731 - val_accuracy: 0.5818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a52e969670>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test)) #it takes 200 min for the completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5771/5771 [==============================] - 78s 14ms/step - loss: 0.9731 - accuracy: 0.5818\n",
      "Test accuracy: 0.5818264484405518\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the accuracy does not improve to much regard to sklearn model (for completed data improve from 0.52 to 0.58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predict function\n",
    "def predict(model, x_test):\n",
    "    predictions = model.predict(x_test)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the sentiment for each review\n",
    "y_pred = predict(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edofa\\AppData\\Local\\Temp\\ipykernel_24444\\1708802160.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_text_keras'] = y_pred\n"
     ]
    }
   ],
   "source": [
    "# Create a new column in the dataframe with the predicted sentiments\n",
    "df['review_text_keras'] = y_pred\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis 5: Tensorflow <br>\n",
    "here's a more detailed explanation of the TensorFlow model: <br>\n",
    "\n",
    "1:Import the TensorFlow library:<br>\n",
    "2:Define the model architecture using the Sequential model: <br>\n",
    "This code creates a model with 3 layers: <br>\n",
    "\n",
    "The Embedding layer takes the input data, which is a numerical representation of the book reviews, and maps each word to a dense vector of fixed size. The input_dim parameter specifies the size of the vocabulary (i.e. the number of unique words in the data), output_dim is the size of the dense vector representation, and input_length is the maximum length of the reviews. <br>\n",
    "\n",
    "The LSTM (Long-Short Term Memory) layer is a type of recurrent neural network layer that is commonly used for sequence data, such as text data. This layer processes the sequences and generates an output representation that summarizes the input sequence. The number 32 represents the number of units in the LSTM layer. <br>\n",
    "\n",
    "The Dense layer is a fully connected layer that takes the output of the LSTM layer and produces 6 output units, each corresponding to a rating class. The softmax activation function is used to ensure that the outputs are probabilities that sum up to 1, so that they can be interpreted as class probabilities.<br>\n",
    "3: Compile the model:<br>\n",
    "The compile method configures the model for training. The loss function is set to sparse_categorical_crossentropy, which is a loss function that is suitable for multi-class classification problems. The optimizer is set to adam, which is a popular optimization algorithm for deep learning models. The metrics argument is set to accuracy, which tells the model to report the accuracy of the predictions during training.<br>\n",
    "4: Train the model:<br>\n",
    "The fit method trains the model on the training data X_train and y_train. The epochs parameter specifies the number of times the model will see the entire training data (10 in this case). The batch_size parameter specifies the number of samples per gradient update (32 in this case). The validation_data argument provides the validation data X_val and y_val to evaluate the model after each epoch.<br>\n",
    "\n",
    "Once the model is trained, the predictions can be obtained using the predict method, as explained in the previous answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "reviews = df['review_text'].values\n",
    "labels = df['rating'].values\n",
    "\n",
    "# Calculate the vocab size\n",
    "vocab = set()\n",
    "for review in reviews:\n",
    "    words = review.split()\n",
    "    for word in words:\n",
    "        vocab.add(word)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a natural language processing (NLP) task, the vocab_size is the size of the vocabulary used in the input text data. The vocabulary is a set of unique words in the input text. In the case of sentiment analysis, vocab_size is the number of unique words in the input reviews.\n",
    "\n",
    "In the code, vocab_size is used as the input dimension of the Embedding layer in the neural network model. The Embedding layer is used to convert the input words into a dense vector representation, which can be fed into other layers in the network.\n",
    "\n",
    "To define the vocab_size, you need to calculate the size of the vocabulary in the input text data.\n",
    "\n",
    "In this code, reviews is a list of the reviews in the dataframe, and vocab is a set that contains all the unique words in the reviews. The size of vocab is calculated using the len() function, and it is stored in vocab_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    }
   ],
   "source": [
    "# Calculate the max length\n",
    "max_length = 0\n",
    "for review in reviews:\n",
    "    words = review.split()\n",
    "    length = len(words)\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "print(length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a natural language processing (NLP) task, max_length refers to the maximum length of the input sequences, such as sentences or documents. In the case of sentiment analysis, max_length is the maximum number of words in a review.\n",
    "\n",
    "The max_length is used to pad or truncate the input sequences to a fixed length, so that all the input sequences have the same length and can be processed by the neural network.\n",
    "\n",
    "To define the max_length, you need to calculate the length of the longest review in your data.\n",
    "\n",
    "In this code, reviews is a list of the reviews in the dataframe, and max_length is initialized to 0. The length of each review is calculated using the len() function, and the longest review is stored in max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the reviews\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "reviews = tokenizer.texts_to_sequences(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad or truncate the reviews to have a length of 100\n",
    "reviews = pad_sequences(reviews, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                    output_dim=100,\n",
    "                    input_length=100))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19235/19235 [==============================] - 5663s 294ms/step - loss: 1.0361 - accuracy: 0.5516\n",
      "Epoch 2/10\n",
      "19235/19235 [==============================] - 5782s 301ms/step - loss: 0.9055 - accuracy: 0.6134\n",
      "Epoch 3/10\n",
      "19235/19235 [==============================] - 5674s 295ms/step - loss: 0.8105 - accuracy: 0.6631\n",
      "Epoch 4/10\n",
      "19235/19235 [==============================] - 5662s 294ms/step - loss: 0.7018 - accuracy: 0.7158\n",
      "Epoch 5/10\n",
      "19235/19235 [==============================] - 5679s 295ms/step - loss: 0.5951 - accuracy: 0.7645\n",
      "Epoch 6/10\n",
      "19235/19235 [==============================] - 5654s 294ms/step - loss: 0.5014 - accuracy: 0.8040\n",
      "Epoch 7/10\n",
      "19235/19235 [==============================] - 5813s 302ms/step - loss: 0.4249 - accuracy: 0.8358\n",
      "Epoch 8/10\n",
      "19235/19235 [==============================] - 5998s 312ms/step - loss: 0.3638 - accuracy: 0.8607\n",
      "Epoch 9/10\n",
      "19235/19235 [==============================] - 5831s 303ms/step - loss: 0.3161 - accuracy: 0.8798\n",
      "Epoch 10/10\n",
      "19235/19235 [==============================] - 5711s 297ms/step - loss: 0.2770 - accuracy: 0.8955\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(reviews, labels, epochs=10, batch_size=32) #It takes 957 minutes for completed data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It reached a accuracy of 0.8955. which is de higher. <br>\n",
    "\n",
    "In this code, the input reviews are first tokenized using the Tokenizer class from the tensorflow.keras.preprocessing.text module. The reviews are then converted to sequences of integers using the texts_to_sequences method. Finally, the input sequences are padded or truncated to have a length of 100 using the pad_sequences function. <br>\n",
    "\n",
    "The model consists of an Embedding layer, an LSTM layer, and a Dense layer with 6 units and a softmax activation function. The model is compiled using the adam optimizer, sparse_categorical_crossentropy loss function, and accuracy as the evaluation metric. Finally, the model is trained on the input data for 10 epochs with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edofa\\AppData\\Local\\Temp\\ipykernel_24444\\1851679411.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_text_tf'] = predicted_classes.numpy()\n"
     ]
    }
   ],
   "source": [
    "# Use the model to make predictions\n",
    "predictions = model.predict(reviews)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "predicted_classes = tf.argmax(predictions, axis=1)\n",
    "\n",
    "# Add the predictions to a new column in the dataframe\n",
    "df['review_text_tf'] = predicted_classes.numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Keras is a high-level API for building deep learning models that runs on top of TensorFlow. <br>\n",
    "\n",
    "When you build a deep learning model using Keras, you are actually building a TensorFlow model, but with a more user-friendly API. In other words, Keras provides a simpler way to create TensorFlow models. This is why the code mentioned tf.keras API, which is a TensorFlow implementation of the Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>review_text_textblob</th>\n",
       "      <th>review_text_nltk</th>\n",
       "      <th>review_text_sktlearn</th>\n",
       "      <th>review_text_keras</th>\n",
       "      <th>review_text_tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>18245960</td>\n",
       "      <td>dfdbb7b0eb5a7e4c26d59a937e2e5feb</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a special book. It started slow for ab...</td>\n",
       "      <td>Sun Jul 30 07:44:10 -0700 2017</td>\n",
       "      <td>Wed Aug 30 00:00:26 -0700 2017</td>\n",
       "      <td>Sat Aug 26 12:05:52 -0700 2017</td>\n",
       "      <td>Tue Aug 15 13:23:18 -0700 2017</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.176222</td>\n",
       "      <td>0.7820</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>28684704</td>\n",
       "      <td>2ede853b14dc4583f96cf5d120af636f</td>\n",
       "      <td>3</td>\n",
       "      <td>A fun, fast paced science fiction thriller. I ...</td>\n",
       "      <td>Tue Nov 15 11:29:22 -0800 2016</td>\n",
       "      <td>Mon Mar 20 23:40:27 -0700 2017</td>\n",
       "      <td>Sat Mar 18 23:22:42 -0700 2017</td>\n",
       "      <td>Fri Mar 17 23:45:40 -0700 2017</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.187156</td>\n",
       "      <td>0.9041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>25884323</td>\n",
       "      <td>332732725863131279a8e345b63ac33e</td>\n",
       "      <td>4</td>\n",
       "      <td>I really enjoyed this book, and there is a lot...</td>\n",
       "      <td>Mon Apr 25 09:31:23 -0700 2016</td>\n",
       "      <td>Mon Apr 25 09:31:23 -0700 2016</td>\n",
       "      <td>Sun Jun 26 00:00:00 -0700 2016</td>\n",
       "      <td>Sat May 28 00:00:00 -0700 2016</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.191298</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>19398490</td>\n",
       "      <td>ea4a220b10e6b5c796dae0e3b970aff1</td>\n",
       "      <td>4</td>\n",
       "      <td>A beautiful story. It is rare to encounter a b...</td>\n",
       "      <td>Sun Jan 03 21:20:46 -0800 2016</td>\n",
       "      <td>Tue Sep 20 23:30:15 -0700 2016</td>\n",
       "      <td>Tue Sep 13 11:51:51 -0700 2016</td>\n",
       "      <td>Sat Aug 20 07:03:03 -0700 2016</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>22318578</td>\n",
       "      <td>5fe9882bfe4b0520a322820c4c55747d</td>\n",
       "      <td>5</td>\n",
       "      <td>5 stars for giving me a better framework for h...</td>\n",
       "      <td>Sun Jun 07 12:50:13 -0700 2015</td>\n",
       "      <td>Wed Mar 22 11:36:58 -0700 2017</td>\n",
       "      <td>Sun Aug 09 00:00:00 -0700 2015</td>\n",
       "      <td>Sun Jun 07 00:00:00 -0700 2015</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.8516</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id   book_id  \\\n",
       "0  8842281e1d1347389f2ab93d60773d4d  18245960   \n",
       "2  8842281e1d1347389f2ab93d60773d4d  28684704   \n",
       "4  8842281e1d1347389f2ab93d60773d4d  25884323   \n",
       "5  8842281e1d1347389f2ab93d60773d4d  19398490   \n",
       "6  8842281e1d1347389f2ab93d60773d4d  22318578   \n",
       "\n",
       "                          review_id  rating  \\\n",
       "0  dfdbb7b0eb5a7e4c26d59a937e2e5feb       5   \n",
       "2  2ede853b14dc4583f96cf5d120af636f       3   \n",
       "4  332732725863131279a8e345b63ac33e       4   \n",
       "5  ea4a220b10e6b5c796dae0e3b970aff1       4   \n",
       "6  5fe9882bfe4b0520a322820c4c55747d       5   \n",
       "\n",
       "                                         review_text  \\\n",
       "0  This is a special book. It started slow for ab...   \n",
       "2  A fun, fast paced science fiction thriller. I ...   \n",
       "4  I really enjoyed this book, and there is a lot...   \n",
       "5  A beautiful story. It is rare to encounter a b...   \n",
       "6  5 stars for giving me a better framework for h...   \n",
       "\n",
       "                       date_added                    date_updated  \\\n",
       "0  Sun Jul 30 07:44:10 -0700 2017  Wed Aug 30 00:00:26 -0700 2017   \n",
       "2  Tue Nov 15 11:29:22 -0800 2016  Mon Mar 20 23:40:27 -0700 2017   \n",
       "4  Mon Apr 25 09:31:23 -0700 2016  Mon Apr 25 09:31:23 -0700 2016   \n",
       "5  Sun Jan 03 21:20:46 -0800 2016  Tue Sep 20 23:30:15 -0700 2016   \n",
       "6  Sun Jun 07 12:50:13 -0700 2015  Wed Mar 22 11:36:58 -0700 2017   \n",
       "\n",
       "                          read_at                      started_at  n_votes  \\\n",
       "0  Sat Aug 26 12:05:52 -0700 2017  Tue Aug 15 13:23:18 -0700 2017       28   \n",
       "2  Sat Mar 18 23:22:42 -0700 2017  Fri Mar 17 23:45:40 -0700 2017       22   \n",
       "4  Sun Jun 26 00:00:00 -0700 2016  Sat May 28 00:00:00 -0700 2016        9   \n",
       "5  Tue Sep 13 11:51:51 -0700 2016  Sat Aug 20 07:03:03 -0700 2016       35   \n",
       "6  Sun Aug 09 00:00:00 -0700 2015  Sun Jun 07 00:00:00 -0700 2015       24   \n",
       "\n",
       "   n_comments  review_text_textblob  review_text_nltk  review_text_sktlearn  \\\n",
       "0           1              0.176222            0.7820                   4.0   \n",
       "2           0              0.187156            0.9041                   NaN   \n",
       "4           1              0.191298            0.9942                   NaN   \n",
       "5           5              0.108333            0.5238                   NaN   \n",
       "6           3              0.500000            0.8516                   4.0   \n",
       "\n",
       "   review_text_keras  review_text_tf  \n",
       "0                  4               5  \n",
       "2                  5               3  \n",
       "4                  4               4  \n",
       "5                  4               4  \n",
       "6                  5               5  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we check out all the sentiment models into de dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_069fb_row0_col0, #T_069fb_row1_col1, #T_069fb_row2_col2, #T_069fb_row3_col3, #T_069fb_row4_col4, #T_069fb_row5_col5, #T_069fb_row6_col6, #T_069fb_row7_col7, #T_069fb_row8_col8 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row0_col1 {\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row0_col2 {\n",
       "  background-color: #5977e3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row0_col3, #T_069fb_row2_col0 {\n",
       "  background-color: #4c66d6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row0_col4, #T_069fb_row6_col3 {\n",
       "  background-color: #4a63d3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row0_col5, #T_069fb_row1_col3, #T_069fb_row5_col3, #T_069fb_row8_col2, #T_069fb_row8_col3 {\n",
       "  background-color: #465ecf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row0_col6, #T_069fb_row7_col0 {\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row0_col7, #T_069fb_row5_col0 {\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row0_col8 {\n",
       "  background-color: #4257c9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row1_col0, #T_069fb_row2_col4, #T_069fb_row2_col7 {\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row1_col2, #T_069fb_row7_col2, #T_069fb_row7_col3 {\n",
       "  background-color: #485fd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row1_col4, #T_069fb_row4_col7 {\n",
       "  background-color: #a1c0ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row1_col5, #T_069fb_row5_col1, #T_069fb_row5_col8, #T_069fb_row7_col5, #T_069fb_row8_col5 {\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row1_col6 {\n",
       "  background-color: #eed0c0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row1_col7 {\n",
       "  background-color: #f59f80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row1_col8, #T_069fb_row8_col1 {\n",
       "  background-color: #cc403a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row2_col1, #T_069fb_row2_col8, #T_069fb_row6_col0, #T_069fb_row8_col0 {\n",
       "  background-color: #3e51c5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row2_col3 {\n",
       "  background-color: #f7b99e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row2_col5, #T_069fb_row3_col1, #T_069fb_row3_col4, #T_069fb_row3_col5, #T_069fb_row3_col6, #T_069fb_row3_col7, #T_069fb_row3_col8, #T_069fb_row4_col0, #T_069fb_row4_col2, #T_069fb_row4_col3 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row2_col6, #T_069fb_row3_col0 {\n",
       "  background-color: #3c4ec2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row3_col2 {\n",
       "  background-color: #f7ba9f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row4_col1 {\n",
       "  background-color: #96b7ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row4_col5 {\n",
       "  background-color: #bbd1f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row4_col6 {\n",
       "  background-color: #98b9ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row4_col8 {\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row5_col2 {\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row5_col4 {\n",
       "  background-color: #c4d5f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row5_col6 {\n",
       "  background-color: #80a3fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row5_col7 {\n",
       "  background-color: #88abfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row6_col1 {\n",
       "  background-color: #efcfbf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row6_col2 {\n",
       "  background-color: #4961d2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row6_col4 {\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row6_col5 {\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row6_col7 {\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row6_col8 {\n",
       "  background-color: #f3c8b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row7_col1 {\n",
       "  background-color: #f59d7e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row7_col4 {\n",
       "  background-color: #abc8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row7_col6 {\n",
       "  background-color: #f6bea4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row7_col8, #T_069fb_row8_col7 {\n",
       "  background-color: #f29274;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_069fb_row8_col4 {\n",
       "  background-color: #a6c4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_069fb_row8_col6 {\n",
       "  background-color: #f2c9b4;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_069fb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_069fb_level0_col0\" class=\"col_heading level0 col0\" >book_id</th>\n",
       "      <th id=\"T_069fb_level0_col1\" class=\"col_heading level0 col1\" >rating</th>\n",
       "      <th id=\"T_069fb_level0_col2\" class=\"col_heading level0 col2\" >n_votes</th>\n",
       "      <th id=\"T_069fb_level0_col3\" class=\"col_heading level0 col3\" >n_comments</th>\n",
       "      <th id=\"T_069fb_level0_col4\" class=\"col_heading level0 col4\" >review_text_textblob</th>\n",
       "      <th id=\"T_069fb_level0_col5\" class=\"col_heading level0 col5\" >review_text_nltk</th>\n",
       "      <th id=\"T_069fb_level0_col6\" class=\"col_heading level0 col6\" >review_text_sktlearn</th>\n",
       "      <th id=\"T_069fb_level0_col7\" class=\"col_heading level0 col7\" >review_text_keras</th>\n",
       "      <th id=\"T_069fb_level0_col8\" class=\"col_heading level0 col8\" >review_text_tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_069fb_level0_row0\" class=\"row_heading level0 row0\" >book_id</th>\n",
       "      <td id=\"T_069fb_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_069fb_row0_col1\" class=\"data row0 col1\" >0.032398</td>\n",
       "      <td id=\"T_069fb_row0_col2\" class=\"data row0 col2\" >0.080987</td>\n",
       "      <td id=\"T_069fb_row0_col3\" class=\"data row0 col3\" >0.029116</td>\n",
       "      <td id=\"T_069fb_row0_col4\" class=\"data row0 col4\" >0.021702</td>\n",
       "      <td id=\"T_069fb_row0_col5\" class=\"data row0 col5\" >0.050687</td>\n",
       "      <td id=\"T_069fb_row0_col6\" class=\"data row0 col6\" >0.036035</td>\n",
       "      <td id=\"T_069fb_row0_col7\" class=\"data row0 col7\" >0.040664</td>\n",
       "      <td id=\"T_069fb_row0_col8\" class=\"data row0 col8\" >0.033670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_069fb_level0_row1\" class=\"row_heading level0 row1\" >rating</th>\n",
       "      <td id=\"T_069fb_row1_col0\" class=\"data row1 col0\" >0.032398</td>\n",
       "      <td id=\"T_069fb_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_069fb_row1_col2\" class=\"data row1 col2\" >0.021954</td>\n",
       "      <td id=\"T_069fb_row1_col3\" class=\"data row1 col3\" >0.009302</td>\n",
       "      <td id=\"T_069fb_row1_col4\" class=\"data row1 col4\" >0.283397</td>\n",
       "      <td id=\"T_069fb_row1_col5\" class=\"data row1 col5\" >0.245600</td>\n",
       "      <td id=\"T_069fb_row1_col6\" class=\"data row1 col6\" >0.585701</td>\n",
       "      <td id=\"T_069fb_row1_col7\" class=\"data row1 col7\" >0.740872</td>\n",
       "      <td id=\"T_069fb_row1_col8\" class=\"data row1 col8\" >0.932705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_069fb_level0_row2\" class=\"row_heading level0 row2\" >n_votes</th>\n",
       "      <td id=\"T_069fb_row2_col0\" class=\"data row2 col0\" >0.080987</td>\n",
       "      <td id=\"T_069fb_row2_col1\" class=\"data row2 col1\" >0.021954</td>\n",
       "      <td id=\"T_069fb_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_069fb_row2_col3\" class=\"data row2 col3\" >0.649478</td>\n",
       "      <td id=\"T_069fb_row2_col4\" class=\"data row2 col4\" >-0.022054</td>\n",
       "      <td id=\"T_069fb_row2_col5\" class=\"data row2 col5\" >0.012510</td>\n",
       "      <td id=\"T_069fb_row2_col6\" class=\"data row2 col6\" >0.026351</td>\n",
       "      <td id=\"T_069fb_row2_col7\" class=\"data row2 col7\" >0.022989</td>\n",
       "      <td id=\"T_069fb_row2_col8\" class=\"data row2 col8\" >0.020503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_069fb_level0_row3\" class=\"row_heading level0 row3\" >n_comments</th>\n",
       "      <td id=\"T_069fb_row3_col0\" class=\"data row3 col0\" >0.029116</td>\n",
       "      <td id=\"T_069fb_row3_col1\" class=\"data row3 col1\" >0.009302</td>\n",
       "      <td id=\"T_069fb_row3_col2\" class=\"data row3 col2\" >0.649478</td>\n",
       "      <td id=\"T_069fb_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "      <td id=\"T_069fb_row3_col4\" class=\"data row3 col4\" >-0.032841</td>\n",
       "      <td id=\"T_069fb_row3_col5\" class=\"data row3 col5\" >0.009532</td>\n",
       "      <td id=\"T_069fb_row3_col6\" class=\"data row3 col6\" >0.019904</td>\n",
       "      <td id=\"T_069fb_row3_col7\" class=\"data row3 col7\" >0.012969</td>\n",
       "      <td id=\"T_069fb_row3_col8\" class=\"data row3 col8\" >0.007611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_069fb_level0_row4\" class=\"row_heading level0 row4\" >review_text_textblob</th>\n",
       "      <td id=\"T_069fb_row4_col0\" class=\"data row4 col0\" >0.021702</td>\n",
       "      <td id=\"T_069fb_row4_col1\" class=\"data row4 col1\" >0.283397</td>\n",
       "      <td id=\"T_069fb_row4_col2\" class=\"data row4 col2\" >-0.022054</td>\n",
       "      <td id=\"T_069fb_row4_col3\" class=\"data row4 col3\" >-0.032841</td>\n",
       "      <td id=\"T_069fb_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_069fb_row4_col5\" class=\"data row4 col5\" >0.391340</td>\n",
       "      <td id=\"T_069fb_row4_col6\" class=\"data row4 col6\" >0.297380</td>\n",
       "      <td id=\"T_069fb_row4_col7\" class=\"data row4 col7\" >0.315831</td>\n",
       "      <td id=\"T_069fb_row4_col8\" class=\"data row4 col8\" >0.298617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_069fb_level0_row5\" class=\"row_heading level0 row5\" >review_text_nltk</th>\n",
       "      <td id=\"T_069fb_row5_col0\" class=\"data row5 col0\" >0.050687</td>\n",
       "      <td id=\"T_069fb_row5_col1\" class=\"data row5 col1\" >0.245600</td>\n",
       "      <td id=\"T_069fb_row5_col2\" class=\"data row5 col2\" >0.012510</td>\n",
       "      <td id=\"T_069fb_row5_col3\" class=\"data row5 col3\" >0.009532</td>\n",
       "      <td id=\"T_069fb_row5_col4\" class=\"data row5 col4\" >0.391340</td>\n",
       "      <td id=\"T_069fb_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "      <td id=\"T_069fb_row5_col6\" class=\"data row5 col6\" >0.228287</td>\n",
       "      <td id=\"T_069fb_row5_col7\" class=\"data row5 col7\" >0.247794</td>\n",
       "      <td id=\"T_069fb_row5_col8\" class=\"data row5 col8\" >0.247916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_069fb_level0_row6\" class=\"row_heading level0 row6\" >review_text_sktlearn</th>\n",
       "      <td id=\"T_069fb_row6_col0\" class=\"data row6 col0\" >0.036035</td>\n",
       "      <td id=\"T_069fb_row6_col1\" class=\"data row6 col1\" >0.585701</td>\n",
       "      <td id=\"T_069fb_row6_col2\" class=\"data row6 col2\" >0.026351</td>\n",
       "      <td id=\"T_069fb_row6_col3\" class=\"data row6 col3\" >0.019904</td>\n",
       "      <td id=\"T_069fb_row6_col4\" class=\"data row6 col4\" >0.297380</td>\n",
       "      <td id=\"T_069fb_row6_col5\" class=\"data row6 col5\" >0.228287</td>\n",
       "      <td id=\"T_069fb_row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "      <td id=\"T_069fb_row6_col7\" class=\"data row6 col7\" >0.655177</td>\n",
       "      <td id=\"T_069fb_row6_col8\" class=\"data row6 col8\" >0.613937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_069fb_level0_row7\" class=\"row_heading level0 row7\" >review_text_keras</th>\n",
       "      <td id=\"T_069fb_row7_col0\" class=\"data row7 col0\" >0.040664</td>\n",
       "      <td id=\"T_069fb_row7_col1\" class=\"data row7 col1\" >0.740872</td>\n",
       "      <td id=\"T_069fb_row7_col2\" class=\"data row7 col2\" >0.022989</td>\n",
       "      <td id=\"T_069fb_row7_col3\" class=\"data row7 col3\" >0.012969</td>\n",
       "      <td id=\"T_069fb_row7_col4\" class=\"data row7 col4\" >0.315831</td>\n",
       "      <td id=\"T_069fb_row7_col5\" class=\"data row7 col5\" >0.247794</td>\n",
       "      <td id=\"T_069fb_row7_col6\" class=\"data row7 col6\" >0.655177</td>\n",
       "      <td id=\"T_069fb_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "      <td id=\"T_069fb_row7_col8\" class=\"data row7 col8\" >0.770453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_069fb_level0_row8\" class=\"row_heading level0 row8\" >review_text_tf</th>\n",
       "      <td id=\"T_069fb_row8_col0\" class=\"data row8 col0\" >0.033670</td>\n",
       "      <td id=\"T_069fb_row8_col1\" class=\"data row8 col1\" >0.932705</td>\n",
       "      <td id=\"T_069fb_row8_col2\" class=\"data row8 col2\" >0.020503</td>\n",
       "      <td id=\"T_069fb_row8_col3\" class=\"data row8 col3\" >0.007611</td>\n",
       "      <td id=\"T_069fb_row8_col4\" class=\"data row8 col4\" >0.298617</td>\n",
       "      <td id=\"T_069fb_row8_col5\" class=\"data row8 col5\" >0.247916</td>\n",
       "      <td id=\"T_069fb_row8_col6\" class=\"data row8 col6\" >0.613937</td>\n",
       "      <td id=\"T_069fb_row8_col7\" class=\"data row8 col7\" >0.770453</td>\n",
       "      <td id=\"T_069fb_row8_col8\" class=\"data row8 col8\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a5ffa6ed30>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Matriz de correlaciones\n",
    "corrMatrix = df.corr() #Genereamos una matriz de correlaciones entre todas las variables\n",
    "corrMatrix.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>review_text_textblob</th>\n",
       "      <th>review_text_nltk</th>\n",
       "      <th>review_text_sktlearn</th>\n",
       "      <th>review_text_keras</th>\n",
       "      <th>review_text_tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032398</td>\n",
       "      <td>0.080987</td>\n",
       "      <td>0.029116</td>\n",
       "      <td>0.021702</td>\n",
       "      <td>0.050687</td>\n",
       "      <td>0.036035</td>\n",
       "      <td>0.040664</td>\n",
       "      <td>0.033670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>0.032398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021954</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.283397</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.585701</td>\n",
       "      <td>0.740872</td>\n",
       "      <td>0.932705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_votes</th>\n",
       "      <td>0.080987</td>\n",
       "      <td>0.021954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.649478</td>\n",
       "      <td>-0.022054</td>\n",
       "      <td>0.012510</td>\n",
       "      <td>0.026351</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.020503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_comments</th>\n",
       "      <td>0.029116</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.649478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.032841</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>0.019904</td>\n",
       "      <td>0.012969</td>\n",
       "      <td>0.007611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_text_textblob</th>\n",
       "      <td>0.021702</td>\n",
       "      <td>0.283397</td>\n",
       "      <td>-0.022054</td>\n",
       "      <td>-0.032841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.391340</td>\n",
       "      <td>0.297380</td>\n",
       "      <td>0.315831</td>\n",
       "      <td>0.298617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_text_nltk</th>\n",
       "      <td>0.050687</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.012510</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>0.391340</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228287</td>\n",
       "      <td>0.247794</td>\n",
       "      <td>0.247916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_text_sktlearn</th>\n",
       "      <td>0.036035</td>\n",
       "      <td>0.585701</td>\n",
       "      <td>0.026351</td>\n",
       "      <td>0.019904</td>\n",
       "      <td>0.297380</td>\n",
       "      <td>0.228287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.655177</td>\n",
       "      <td>0.613937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_text_keras</th>\n",
       "      <td>0.040664</td>\n",
       "      <td>0.740872</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.012969</td>\n",
       "      <td>0.315831</td>\n",
       "      <td>0.247794</td>\n",
       "      <td>0.655177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.770453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_text_tf</th>\n",
       "      <td>0.033670</td>\n",
       "      <td>0.932705</td>\n",
       "      <td>0.020503</td>\n",
       "      <td>0.007611</td>\n",
       "      <td>0.298617</td>\n",
       "      <td>0.247916</td>\n",
       "      <td>0.613937</td>\n",
       "      <td>0.770453</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       book_id    rating   n_votes  n_comments  \\\n",
       "book_id               1.000000  0.032398  0.080987    0.029116   \n",
       "rating                0.032398  1.000000  0.021954    0.009302   \n",
       "n_votes               0.080987  0.021954  1.000000    0.649478   \n",
       "n_comments            0.029116  0.009302  0.649478    1.000000   \n",
       "review_text_textblob  0.021702  0.283397 -0.022054   -0.032841   \n",
       "review_text_nltk      0.050687  0.245600  0.012510    0.009532   \n",
       "review_text_sktlearn  0.036035  0.585701  0.026351    0.019904   \n",
       "review_text_keras     0.040664  0.740872  0.022989    0.012969   \n",
       "review_text_tf        0.033670  0.932705  0.020503    0.007611   \n",
       "\n",
       "                      review_text_textblob  review_text_nltk  \\\n",
       "book_id                           0.021702          0.050687   \n",
       "rating                            0.283397          0.245600   \n",
       "n_votes                          -0.022054          0.012510   \n",
       "n_comments                       -0.032841          0.009532   \n",
       "review_text_textblob              1.000000          0.391340   \n",
       "review_text_nltk                  0.391340          1.000000   \n",
       "review_text_sktlearn              0.297380          0.228287   \n",
       "review_text_keras                 0.315831          0.247794   \n",
       "review_text_tf                    0.298617          0.247916   \n",
       "\n",
       "                      review_text_sktlearn  review_text_keras  review_text_tf  \n",
       "book_id                           0.036035           0.040664        0.033670  \n",
       "rating                            0.585701           0.740872        0.932705  \n",
       "n_votes                           0.026351           0.022989        0.020503  \n",
       "n_comments                        0.019904           0.012969        0.007611  \n",
       "review_text_textblob              0.297380           0.315831        0.298617  \n",
       "review_text_nltk                  0.228287           0.247794        0.247916  \n",
       "review_text_sktlearn              1.000000           0.655177        0.613937  \n",
       "review_text_keras                 0.655177           1.000000        0.770453  \n",
       "review_text_tf                    0.613937           0.770453        1.000000  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrMatrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 5 model of sentiment analysis, textblob and nltk are independent of rating, and sklearn, keras and tf are dependent of rating. The regression is high but we have to consider it because the 70% of the data was used for train. So it could be overfitting. For example for Keras the test data just gave us 50% de accuracy but if we use the whole data (train+test) it gives us almost 79%. The best results is by far given by Tensor flow with 97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184653, 16)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the string variables\n",
    "labelencoder = LabelEncoder()\n",
    "df['user_id'] = labelencoder.fit_transform(df['user_id'])\n",
    "df['book_id'] = labelencoder.fit_transform(df['book_id'])\n",
    "df['review_id'] = labelencoder.fit_transform(df['review_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target variable\n",
    "X = df.drop(['book_id', 'review_id', 'user_id',\"rating\",'date_added','date_updated','read_at','started_at', 'review_text'], axis=1)\n",
    "y = df[\"rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a linear regression model to the training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the star ratings on the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the predicted values to the nearest integer to obtain class labels\n",
    "y_pred_class = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9234043931861067\n"
     ]
    }
   ],
   "source": [
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a good F1 score.\n"
     ]
    }
   ],
   "source": [
    "# Check if the F1 score is over 70%\n",
    "if f1 > 0.7:\n",
    "    print(\"The model has a good F1 score.\")\n",
    "else:\n",
    "    print(\"The model needs improvement. Try using a different model or adding more features.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo 2 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a KNN model on the training data\n",
    "knn = KNeighborsRegressor(n_neighbors=50)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the ratings on the test data\n",
    "y_pred_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Model\n",
      "MSE:  0.21391700197665917\n",
      "R-squared:  0.8294946219356624\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MSE and R-squared for the KNN model\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "print(\"KNN Model\")\n",
    "print(\"MSE: \", mse_knn)\n",
    "print(\"R-squared: \", r2_knn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MSE, a smaller value indicates a better fit of the model to the data. A value of 0 means the model perfectly predicts the target. However, the MSE is sensitive to outliers, so it may not always be a reliable measure of the model's performance. <br>\n",
    "\n",
    "For the R-squared, the value ranges from 0 to 1, with higher values indicating a better fit of the model to the data. A value of 1 means the model perfectly predicts the target. However, having a high R-squared does not always mean that the model is a good fit, as it can also increase with the addition of irrelevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation MSE:  0.2094075878845219\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation on the KNN model\n",
    "cv_scores_knn = cross_val_score(knn, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_knn = np.abs(cv_scores_knn)\n",
    "print(\"Cross-Validation MSE: \", cv_scores_knn.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=0)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Random Forest model on the training data\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "rf.fit(X_train, y_train) #it takes 9 minutes with the completed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the ratings on the test data\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model\n",
      "MSE:  0.19311277278295988\n",
      "R-squared:  0.8460769082019773\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MSE and R-squared for the Random Forest model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Model\")\n",
    "print(\"MSE: \", mse_rf)\n",
    "print(\"R-squared: \", r2_rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the random forest is slightly better performance but still far away from a satisfying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation MSE:  0.1846946963814765\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation on the Random Forest model\n",
    "cv_scores_rf = cross_val_score(rf, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_rf = np.abs(cv_scores_rf)\n",
    "print(\"Cross-Validation MSE: \", cv_scores_rf.mean()) ##it takes 35 minutes with the completed data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the Mean Squared Error (MSE) in cross-validation, the better the performance of the model. A good MSE score indicates that the model is making predictions that are close to the true values, while a high MSE score suggests that the model is making significant errors in its predictions. <br>\n",
    "\n",
    "A MSE of 0 indicates a perfect fit, while a high MSE value suggests a poor fit. However, it is important to keep in mind that our range of working is between 1 to 5. <br>\n",
    "\n",
    "In general, you can compare the cross-validation MSE of your model with the MSE of a baseline model or with the MSE of other models that you have tried to see if the performance of your model is good or bad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e9b2ac707fb302344000458e68859b5591fb383e6b1a3438783dffd073423d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
