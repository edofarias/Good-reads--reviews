{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Reads\n",
    "\n",
    "Dataset Description\n",
    "This dataset contains more than 1.3M book reviews about 25,475 books and 18,892 users , which is a review subset for spoiler detection, where each book/user has at least one associated spoiler review.\n",
    "\n",
    "Goodreads Books Review Rating Prediction\n",
    "Reviews are a good way to judge the quality of any product, whether it's books, clothes, technology, or anything else. When you want to buy something online these days, the first thing that comes to mind is the reviews from past buyers and the overall rating the product has received.\n",
    "Reader feedback, whether positive or negative, five stars or one star, will encourage the product owner to make improvements.\n",
    "Reader connection and engagement will be encouraged by book reviews, whether they be left on Amazon, Goodreads, or social media. Readers must determine whether or not other readers are enjoying the book.\n",
    "\n",
    "In this competition you will work with a challenging dataset consisting reviews from the Goodreads book review website, and a variety of attributes describing the items. and you have to predict review rating which ranges from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3177122387.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [79]\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install spacy\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#pip install nltk\n",
    "#pip install spacy\n",
    "# pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from textblob import TextBlob #sentiment anlysis library\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer #another sentiment analysis library\n",
    "#import spacy #another sentiment analysis library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #the following 4 libraries are for a customize model of sentiment with sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "from keras.preprocessing.text import Tokenizer #the following 4 libraries are for a customize model of sentiment with deep learning: keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf # This library is for a customize model of sentiement with deep learning: Tensor flow\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>898402</td>\n",
       "      <td>ed556b92506c3452b42fffed31697a1a</td>\n",
       "      <td>25125233</td>\n",
       "      <td>182718faad99666b70f73f3b7ffbbdb7</td>\n",
       "      <td>3</td>\n",
       "      <td>Jessica Broussard has only ever had her father...</td>\n",
       "      <td>Thu Nov 19 07:48:55 -0800 2015</td>\n",
       "      <td>Thu Nov 19 11:31:55 -0800 2015</td>\n",
       "      <td>Wed Nov 18 00:00:00 -0800 2015</td>\n",
       "      <td>Tue Nov 17 00:00:00 -0800 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>853518</td>\n",
       "      <td>fcf6bca39e8f5333ba018b0e146ccfec</td>\n",
       "      <td>6837103</td>\n",
       "      <td>228c47ca18ed4c1598ae3c9214530b6b</td>\n",
       "      <td>5</td>\n",
       "      <td>Set in the late 1700s and early 1800s, the sto...</td>\n",
       "      <td>Tue Dec 06 13:19:29 -0800 2011</td>\n",
       "      <td>Mon Dec 19 11:28:19 -0800 2011</td>\n",
       "      <td>Mon Dec 19 00:00:00 -0800 2011</td>\n",
       "      <td>Tue Dec 06 00:00:00 -0800 2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>366741</td>\n",
       "      <td>b8f6f163c2161555c6d887632b2ff4a2</td>\n",
       "      <td>17948485</td>\n",
       "      <td>6137b1fe0159b7eaa56a04293a00fd49</td>\n",
       "      <td>5</td>\n",
       "      <td>This book is the bomb! Love every single page ...</td>\n",
       "      <td>Fri Jul 26 17:43:19 -0700 2013</td>\n",
       "      <td>Sat Aug 10 20:06:19 -0700 2013</td>\n",
       "      <td>Sat Jul 27 00:00:00 -0700 2013</td>\n",
       "      <td>Fri Jul 26 00:00:00 -0700 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>476233</td>\n",
       "      <td>33162c8e64b16bcbddc9808f3c716342</td>\n",
       "      <td>18405</td>\n",
       "      <td>c5b3dc0c0416d850380d80f5304be91f</td>\n",
       "      <td>5</td>\n",
       "      <td>Wherein I attempt to write a review using all ...</td>\n",
       "      <td>Wed Jun 30 08:01:44 -0700 2010</td>\n",
       "      <td>Tue Dec 31 06:07:21 -0800 2013</td>\n",
       "      <td>Fri Feb 18 00:00:00 -0800 2011</td>\n",
       "      <td>Sat Feb 12 00:00:00 -0800 2011</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>856723</td>\n",
       "      <td>37d8353490e210e2b3766336be99ebd4</td>\n",
       "      <td>26218626</td>\n",
       "      <td>234b51de9a79dfd51b5dc2b48df972ec</td>\n",
       "      <td>3</td>\n",
       "      <td>I can't believe I have only one volume left. T...</td>\n",
       "      <td>Wed Jun 29 19:48:42 -0700 2016</td>\n",
       "      <td>Thu Mar 30 07:18:31 -0700 2017</td>\n",
       "      <td>Thu Mar 30 07:18:31 -0700 2017</td>\n",
       "      <td>Thu Mar 30 00:00:00 -0700 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                           user_id   book_id  \\\n",
       "0      898402  ed556b92506c3452b42fffed31697a1a  25125233   \n",
       "1      853518  fcf6bca39e8f5333ba018b0e146ccfec   6837103   \n",
       "2      366741  b8f6f163c2161555c6d887632b2ff4a2  17948485   \n",
       "3      476233  33162c8e64b16bcbddc9808f3c716342     18405   \n",
       "4      856723  37d8353490e210e2b3766336be99ebd4  26218626   \n",
       "\n",
       "                          review_id  rating  \\\n",
       "0  182718faad99666b70f73f3b7ffbbdb7       3   \n",
       "1  228c47ca18ed4c1598ae3c9214530b6b       5   \n",
       "2  6137b1fe0159b7eaa56a04293a00fd49       5   \n",
       "3  c5b3dc0c0416d850380d80f5304be91f       5   \n",
       "4  234b51de9a79dfd51b5dc2b48df972ec       3   \n",
       "\n",
       "                                         review_text  \\\n",
       "0  Jessica Broussard has only ever had her father...   \n",
       "1  Set in the late 1700s and early 1800s, the sto...   \n",
       "2  This book is the bomb! Love every single page ...   \n",
       "3  Wherein I attempt to write a review using all ...   \n",
       "4  I can't believe I have only one volume left. T...   \n",
       "\n",
       "                       date_added                    date_updated  \\\n",
       "0  Thu Nov 19 07:48:55 -0800 2015  Thu Nov 19 11:31:55 -0800 2015   \n",
       "1  Tue Dec 06 13:19:29 -0800 2011  Mon Dec 19 11:28:19 -0800 2011   \n",
       "2  Fri Jul 26 17:43:19 -0700 2013  Sat Aug 10 20:06:19 -0700 2013   \n",
       "3  Wed Jun 30 08:01:44 -0700 2010  Tue Dec 31 06:07:21 -0800 2013   \n",
       "4  Wed Jun 29 19:48:42 -0700 2016  Thu Mar 30 07:18:31 -0700 2017   \n",
       "\n",
       "                          read_at                      started_at  n_votes  \\\n",
       "0  Wed Nov 18 00:00:00 -0800 2015  Tue Nov 17 00:00:00 -0800 2015        0   \n",
       "1  Mon Dec 19 00:00:00 -0800 2011  Tue Dec 06 00:00:00 -0800 2011        0   \n",
       "2  Sat Jul 27 00:00:00 -0700 2013  Fri Jul 26 00:00:00 -0700 2013        0   \n",
       "3  Fri Feb 18 00:00:00 -0800 2011  Sat Feb 12 00:00:00 -0800 2011       46   \n",
       "4  Thu Mar 30 07:18:31 -0700 2017  Thu Mar 30 00:00:00 -0700 2017        0   \n",
       "\n",
       "   n_comments  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3          14  \n",
       "4           0  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "path = \".\\goodreads-books-reviews-290312\\goodreads_train_sample.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"Unnamed: 0\"], axis=1) #this just when I upload the sample of 10% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         object\n",
       "book_id          int64\n",
       "review_id       object\n",
       "rating           int64\n",
       "review_text     object\n",
       "date_added      object\n",
       "date_updated    object\n",
       "read_at         object\n",
       "started_at      object\n",
       "n_votes          int64\n",
       "n_comments       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check type var\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         90000\n",
       "book_id         90000\n",
       "review_id       90000\n",
       "rating          90000\n",
       "review_text     90000\n",
       "date_added      90000\n",
       "date_updated    90000\n",
       "read_at         80742\n",
       "started_at      62664\n",
       "n_votes         90000\n",
       "n_comments      90000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id             0\n",
       "book_id             0\n",
       "review_id           0\n",
       "rating              0\n",
       "review_text         0\n",
       "date_added          0\n",
       "date_updated        0\n",
       "read_at          9258\n",
       "started_at      27336\n",
       "n_votes             0\n",
       "n_comments          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>61662</td>\n",
       "      <td>6.166200e+04</td>\n",
       "      <td>61662</td>\n",
       "      <td>61662.000000</td>\n",
       "      <td>61662</td>\n",
       "      <td>61662</td>\n",
       "      <td>61662</td>\n",
       "      <td>61662</td>\n",
       "      <td>61662</td>\n",
       "      <td>61662.000000</td>\n",
       "      <td>61662.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61381</td>\n",
       "      <td>61641</td>\n",
       "      <td>61513</td>\n",
       "      <td>28535</td>\n",
       "      <td>6242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>843a44e2499ba9362b47a089b0b0ce75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182718faad99666b70f73f3b7ffbbdb7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Review to come.</td>\n",
       "      <td>Sat Jul 29 09:26:28 -0700 2017</td>\n",
       "      <td>Fri Jul 19 06:15:59 -0700 2013</td>\n",
       "      <td>Fri Jan 01 00:00:00 -0800 2016</td>\n",
       "      <td>Sun Jan 01 00:00:00 -0800 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>173</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.457580e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.793876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.631329</td>\n",
       "      <td>1.152833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.167256e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.128300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.489702</td>\n",
       "      <td>5.975086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.746506e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.576785e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.185739e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.632868e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1251.000000</td>\n",
       "      <td>468.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 user_id       book_id  \\\n",
       "count                              61662  6.166200e+04   \n",
       "unique                              8906           NaN   \n",
       "top     843a44e2499ba9362b47a089b0b0ce75           NaN   \n",
       "freq                                 173           NaN   \n",
       "mean                                 NaN  1.457580e+07   \n",
       "std                                  NaN  9.167256e+06   \n",
       "min                                  NaN  1.000000e+00   \n",
       "25%                                  NaN  7.746506e+06   \n",
       "50%                                  NaN  1.576785e+07   \n",
       "75%                                  NaN  2.185739e+07   \n",
       "max                                  NaN  3.632868e+07   \n",
       "\n",
       "                               review_id        rating      review_text  \\\n",
       "count                              61662  61662.000000            61662   \n",
       "unique                             61662           NaN            61381   \n",
       "top     182718faad99666b70f73f3b7ffbbdb7           NaN  Review to come.   \n",
       "freq                                   1           NaN               43   \n",
       "mean                                 NaN      3.793876              NaN   \n",
       "std                                  NaN      1.128300              NaN   \n",
       "min                                  NaN      0.000000              NaN   \n",
       "25%                                  NaN      3.000000              NaN   \n",
       "50%                                  NaN      4.000000              NaN   \n",
       "75%                                  NaN      5.000000              NaN   \n",
       "max                                  NaN      5.000000              NaN   \n",
       "\n",
       "                            date_added                    date_updated  \\\n",
       "count                            61662                           61662   \n",
       "unique                           61641                           61513   \n",
       "top     Sat Jul 29 09:26:28 -0700 2017  Fri Jul 19 06:15:59 -0700 2013   \n",
       "freq                                 2                               4   \n",
       "mean                               NaN                             NaN   \n",
       "std                                NaN                             NaN   \n",
       "min                                NaN                             NaN   \n",
       "25%                                NaN                             NaN   \n",
       "50%                                NaN                             NaN   \n",
       "75%                                NaN                             NaN   \n",
       "max                                NaN                             NaN   \n",
       "\n",
       "                               read_at                      started_at  \\\n",
       "count                            61662                           61662   \n",
       "unique                           28535                            6242   \n",
       "top     Fri Jan 01 00:00:00 -0800 2016  Sun Jan 01 00:00:00 -0800 2017   \n",
       "freq                                48                              80   \n",
       "mean                               NaN                             NaN   \n",
       "std                                NaN                             NaN   \n",
       "min                                NaN                             NaN   \n",
       "25%                                NaN                             NaN   \n",
       "50%                                NaN                             NaN   \n",
       "75%                                NaN                             NaN   \n",
       "max                                NaN                             NaN   \n",
       "\n",
       "             n_votes    n_comments  \n",
       "count   61662.000000  61662.000000  \n",
       "unique           NaN           NaN  \n",
       "top              NaN           NaN  \n",
       "freq             NaN           NaN  \n",
       "mean        3.631329      1.152833  \n",
       "std        15.489702      5.975086  \n",
       "min         0.000000      0.000000  \n",
       "25%         0.000000      0.000000  \n",
       "50%         0.000000      0.000000  \n",
       "75%         2.000000      0.000000  \n",
       "max      1251.000000    468.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis 1: Textblob\n",
    "In this code, we use the pandas library to read the book review data into a dataframe, and the TextBlob library to perform sentiment analysis. We apply the sentiment analysis to each review in the \"review\" column of the dataframe and store the results in a new column called \"sentiment\". The polarity score of each review ranges from -1 (most negative) to 1 (most positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the dataframe to store the sentiment scores\n",
    "df['review_text_textblob'] = df['review_text'].apply(lambda x: TextBlob(x).sentiment.polarity) #it takes 14 minutes with the completed data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis 2: Nltk\n",
    "In this code, we use the pandas library to read the book review data into a dataframe, and the SentimentIntensityAnalyzer from the nltk library to perform sentiment analysis. We apply the sentiment analysis to each review in the \"review\" column of the dataframe and store the results in a new column called \"sentiment\". The polarity score of each review ranges from -1 (most negative) to 1 (most positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\edofa\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SentimentIntensityAnalyzer\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the dataframe to store the sentiment scores\n",
    "df['review_text_nltk'] = df['review_text'].apply(lambda x: sia.polarity_scores(x)['compound']) #it takes 2 min with the sample data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis 3: Spacy\n",
    "In this code, we use the pandas library to read the book review data into a dataframe, and the spacy library to perform sentiment analysis. We apply the sentiment analysis to each review in the \"review\" column of the dataframe and store the results in a new column called \"sentiment\". The sentiment score of each review is either 1 (positive) or -1 (negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\edofa\\OneDrive\\Escritorio\\evalueserve\\Good reads  reviews\\Goodreads.ipynb Celda 17\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/edofa/OneDrive/Escritorio/evalueserve/Good%20reads%20%20reviews/Goodreads.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the English language model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/edofa/OneDrive/Escritorio/evalueserve/Good%20reads%20%20reviews/Goodreads.ipynb#X61sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the English language model\n",
    "#nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get the sentiment score of a review\n",
    "#def get_sentiment(review):\n",
    "    #doc = nlp(review)\n",
    "    #if doc.cats['POSITIVE'] > doc.cats['NEGATIVE']:\n",
    "        #return 1\n",
    "    #else:\n",
    "        #return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the dataframe to store the sentiment scores\n",
    "#df['review_text_spacy'] = df['review_text'].apply(lambda x: get_sentiment(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis 4: Customize model with sklearn\n",
    "\n",
    "In this code, we use the pandas library to read the book review data into a dataframe and split it into training and testing sets.\n",
    "\n",
    "Next, we use the TfidfVectorizer from scikit-learn to convert the text data into numerical features using the Tf-idf representation.\n",
    "\n",
    "We then train a linear support vector machine (SVM) classifier using the LinearSVC class from scikit-learn on the training data.\n",
    "\n",
    "We use the trained model to make predictions on the test data and evaluate its accuracy using the accuracy_score function from scikit-learn.\n",
    "\n",
    "Finally, we add the prediction results as a new column in the dataframe using df.loc[] to assign the values only for the rows in the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review_text'], df['rating'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text data into numerical features using Tf-idf\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a linear support vector machine (SVM) classifier\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4888912914211579\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the prediction results as a new column in the dataframe\n",
    "df['review_text_sktlearn'] = np.nan\n",
    "df.loc[X_test.index, 'review_text_sktlearn'] = y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis 5: Keras\n",
    "\n",
    "Here's a step-by-step explanation of the process for training a multi-class sentiment analysis model in Keras to predict the rating of a book review:\n",
    "\n",
    "Load the data: The first step is to load the book review data into a pandas dataframe. In this example, the data is loaded from a CSV file using the read_csv function.\n",
    "\n",
    "Preprocess the text data: The next step is to preprocess the text data. To do this, we use the Tokenizer class from Keras. We first fit the tokenizer on the text data, which creates a vocabulary of the most common words in the text. Then, we use the tokenizer to convert the text data into numerical sequences, where each word is represented by its index in the vocabulary. Finally, we use the pad_sequences function from Keras to pad the sequences to a fixed length of 100.\n",
    "\n",
    "Convert the rating data into categorical labels: We convert the rating data into categorical labels using the to_categorical function from Keras. This function converts the rating data into a one-hot encoded representation, where each rating is represented as a vector of 6 elements with a 1 in the index corresponding to the rating and 0s in all other elements.\n",
    "\n",
    "Split the data into training and testing sets: We split the preprocessed data into training and testing sets using the train_test_split function from scikit-learn. This function splits the data into two sets, one for training the model and one for evaluating its performance.\n",
    "\n",
    "Define the model architecture: We define the model architecture using the Sequential model from Keras. We add an Embedding layer, which maps the word indices to dense vectors of fixed size. The dense vectors are used as input to the next layer. We add a GRU (Gated Recurrent Unit) layer, which is a type of recurrent neural network that processes sequences of input data. The GRU layer allows the model to take into account the context and dependencies between words in the review. Finally, we add a Dense layer with a softmax activation function, which outputs the probability of each rating class.\n",
    "\n",
    "Compile the model: We compile the model using the compile method and specify the loss function, optimizer, and evaluation metrics. The loss function measures the difference between the predicted and actual ratings. The optimizer updates the model weights to minimize the loss. The evaluation metrics measure the performance of the model.\n",
    "\n",
    "Train the model: We train the model using the fit method. The model is trained on the training data, and the model weights are updated to minimize the loss function. The model is trained for 10 epochs, where an epoch is one pass over the training data. The batch size is set to 32, which means that the model updates its weights after processing 32 reviews at a time.\n",
    "\n",
    "Evaluate the model: Finally, we evaluate the model on the test data using the evaluate method. The evaluate method returns the loss and accuracy of the model on the test data.\n",
    "\n",
    "By following these steps, we can train a multi-class sentiment analysis model in Keras to predict the rating of a book review based on its text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['review_text'])\n",
    "X = tokenizer.texts_to_sequences(df['review_text'])\n",
    "X = pad_sequences(X, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the rating data into categorical labels\n",
    "y = to_categorical(df['rating'], num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 128, input_length=100))\n",
    "model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1349/1349 [==============================] - 131s 95ms/step - loss: 1.2230 - accuracy: 0.4669 - val_loss: 1.0902 - val_accuracy: 0.5312\n",
      "Epoch 2/10\n",
      "1349/1349 [==============================] - 132s 98ms/step - loss: 1.0447 - accuracy: 0.5562 - val_loss: 1.0743 - val_accuracy: 0.5379\n",
      "Epoch 3/10\n",
      "1349/1349 [==============================] - 135s 100ms/step - loss: 0.9679 - accuracy: 0.5931 - val_loss: 1.0857 - val_accuracy: 0.5396\n",
      "Epoch 4/10\n",
      "1349/1349 [==============================] - 139s 103ms/step - loss: 0.8993 - accuracy: 0.6259 - val_loss: 1.1321 - val_accuracy: 0.5247\n",
      "Epoch 5/10\n",
      "1349/1349 [==============================] - 143s 106ms/step - loss: 0.8331 - accuracy: 0.6562 - val_loss: 1.1696 - val_accuracy: 0.5180\n",
      "Epoch 6/10\n",
      "1349/1349 [==============================] - 140s 104ms/step - loss: 0.7704 - accuracy: 0.6850 - val_loss: 1.2254 - val_accuracy: 0.5040\n",
      "Epoch 7/10\n",
      "1349/1349 [==============================] - 137s 101ms/step - loss: 0.7092 - accuracy: 0.7139 - val_loss: 1.3186 - val_accuracy: 0.4999\n",
      "Epoch 8/10\n",
      "1349/1349 [==============================] - 138s 102ms/step - loss: 0.6500 - accuracy: 0.7381 - val_loss: 1.4013 - val_accuracy: 0.4875\n",
      "Epoch 9/10\n",
      "1349/1349 [==============================] - 135s 100ms/step - loss: 0.5916 - accuracy: 0.7659 - val_loss: 1.5234 - val_accuracy: 0.4878\n",
      "Epoch 10/10\n",
      "1349/1349 [==============================] - 132s 98ms/step - loss: 0.5418 - accuracy: 0.7870 - val_loss: 1.6212 - val_accuracy: 0.4842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5222c7d60>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test)) #it takes 22 min for the sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579/579 [==============================] - 7s 12ms/step - loss: 1.6212 - accuracy: 0.4842\n",
      "Test accuracy: 0.48418834805488586\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the accuracy does not improbe to much regard to sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predict function\n",
    "def predict(model, x_test):\n",
    "    predictions = model.predict(x_test)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the sentiment for each review\n",
    "y_pred = predict(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the dataframe with the predicted sentiments\n",
    "df['review_text_keras'] = y_pred\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis 5: Tensorflow\n",
    "here's a more detailed explanation of the TensorFlow model:\n",
    "\n",
    "1:Import the TensorFlow library:\n",
    "2:Define the model architecture using the Sequential model:\n",
    "This code creates a model with 3 layers:\n",
    "\n",
    "The Embedding layer takes the input data, which is a numerical representation of the book reviews, and maps each word to a dense vector of fixed size. The input_dim parameter specifies the size of the vocabulary (i.e. the number of unique words in the data), output_dim is the size of the dense vector representation, and input_length is the maximum length of the reviews.\n",
    "\n",
    "The LSTM (Long-Short Term Memory) layer is a type of recurrent neural network layer that is commonly used for sequence data, such as text data. This layer processes the sequences and generates an output representation that summarizes the input sequence. The number 32 represents the number of units in the LSTM layer.\n",
    "\n",
    "The Dense layer is a fully connected layer that takes the output of the LSTM layer and produces 6 output units, each corresponding to a rating class. The softmax activation function is used to ensure that the outputs are probabilities that sum up to 1, so that they can be interpreted as class probabilities.\n",
    "3: Compile the model:\n",
    "The compile method configures the model for training. The loss function is set to sparse_categorical_crossentropy, which is a loss function that is suitable for multi-class classification problems. The optimizer is set to adam, which is a popular optimization algorithm for deep learning models. The metrics argument is set to accuracy, which tells the model to report the accuracy of the predictions during training.\n",
    "4: Train the model:\n",
    "The fit method trains the model on the training data X_train and y_train. The epochs parameter specifies the number of times the model will see the entire training data (10 in this case). The batch_size parameter specifies the number of samples per gradient update (32 in this case). The validation_data argument provides the validation data X_val and y_val to evaluate the model after each epoch.\n",
    "\n",
    "Once the model is trained, the predictions can be obtained using the predict method, as explained in the previous answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "reviews = df['review_text'].values\n",
    "labels = df['rating'].values\n",
    "\n",
    "# Calculate the vocab size\n",
    "vocab = set()\n",
    "for review in reviews:\n",
    "    words = review.split()\n",
    "    for word in words:\n",
    "        vocab.add(word)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a natural language processing (NLP) task, the vocab_size is the size of the vocabulary used in the input text data. The vocabulary is a set of unique words in the input text. In the case of sentiment analysis, vocab_size is the number of unique words in the input reviews.\n",
    "\n",
    "In the code, vocab_size is used as the input dimension of the Embedding layer in the neural network model. The Embedding layer is used to convert the input words into a dense vector representation, which can be fed into other layers in the network.\n",
    "\n",
    "To define the vocab_size, you need to calculate the size of the vocabulary in the input text data.\n",
    "\n",
    "In this code, reviews is a list of the reviews in the dataframe, and vocab is a set that contains all the unique words in the reviews. The size of vocab is calculated using the len() function, and it is stored in vocab_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the max length\n",
    "max_length = 0\n",
    "for review in reviews:\n",
    "    words = review.split()\n",
    "    length = len(words)\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "print(length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a natural language processing (NLP) task, max_length refers to the maximum length of the input sequences, such as sentences or documents. In the case of sentiment analysis, max_length is the maximum number of words in a review.\n",
    "\n",
    "The max_length is used to pad or truncate the input sequences to a fixed length, so that all the input sequences have the same length and can be processed by the neural network.\n",
    "\n",
    "To define the max_length, you need to calculate the length of the longest review in your data.\n",
    "\n",
    "In this code, reviews is a list of the reviews in the dataframe, and max_length is initialized to 0. The length of each review is calculated using the len() function, and the longest review is stored in max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the reviews\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "reviews = tokenizer.texts_to_sequences(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad or truncate the reviews to have a length of 100\n",
    "reviews = pad_sequences(reviews, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                    output_dim=100,\n",
    "                    input_length=100))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1927/1927 [==============================] - 233s 120ms/step - loss: 1.2153 - accuracy: 0.4701\n",
      "Epoch 2/10\n",
      "1927/1927 [==============================] - 238s 123ms/step - loss: 0.9979 - accuracy: 0.5846\n",
      "Epoch 3/10\n",
      "1927/1927 [==============================] - 236s 122ms/step - loss: 0.8250 - accuracy: 0.6698\n",
      "Epoch 4/10\n",
      "1927/1927 [==============================] - 229s 119ms/step - loss: 0.6455 - accuracy: 0.7524\n",
      "Epoch 5/10\n",
      "1927/1927 [==============================] - 219s 114ms/step - loss: 0.4816 - accuracy: 0.8201\n",
      "Epoch 6/10\n",
      "1927/1927 [==============================] - 218s 113ms/step - loss: 0.3494 - accuracy: 0.8721\n",
      "Epoch 7/10\n",
      "1927/1927 [==============================] - 218s 113ms/step - loss: 0.2623 - accuracy: 0.9052\n",
      "Epoch 8/10\n",
      "1927/1927 [==============================] - 217s 113ms/step - loss: 0.1977 - accuracy: 0.9284\n",
      "Epoch 9/10\n",
      "1927/1927 [==============================] - 219s 114ms/step - loss: 0.1552 - accuracy: 0.9449\n",
      "Epoch 10/10\n",
      "1927/1927 [==============================] - 218s 113ms/step - loss: 0.1216 - accuracy: 0.9570\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(reviews, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, the input reviews are first tokenized using the Tokenizer class from the tensorflow.keras.preprocessing.text module. The reviews are then converted to sequences of integers using the texts_to_sequences method. Finally, the input sequences are padded or truncated to have a length of 100 using the pad_sequences function.\n",
    "\n",
    "The model consists of an Embedding layer, an LSTM layer, and a Dense layer with 6 units and a softmax activation function. The model is compiled using the adam optimizer, sparse_categorical_crossentropy loss function, and accuracy as the evaluation metric. Finally, the model is trained on the input data for 10 epochs with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions\n",
    "predictions = model.predict(reviews)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "predicted_classes = tf.argmax(predictions, axis=1)\n",
    "\n",
    "# Add the predictions to a new column in the dataframe\n",
    "df['review_text_tf'] = predicted_classes.numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Keras is a high-level API for building deep learning models that runs on top of TensorFlow.\n",
    "\n",
    "When you build a deep learning model using Keras, you are actually building a TensorFlow model, but with a more user-friendly API. In other words, Keras provides a simpler way to create TensorFlow models. This is why the code mentioned in my previous answer uses the tf.keras API, which is a TensorFlow implementation of the Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>review_text_textblob</th>\n",
       "      <th>review_text_nltk</th>\n",
       "      <th>review_text_sktlearn</th>\n",
       "      <th>review_text_keras</th>\n",
       "      <th>review_text_tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ed556b92506c3452b42fffed31697a1a</td>\n",
       "      <td>25125233</td>\n",
       "      <td>182718faad99666b70f73f3b7ffbbdb7</td>\n",
       "      <td>3</td>\n",
       "      <td>Jessica Broussard has only ever had her father...</td>\n",
       "      <td>Thu Nov 19 07:48:55 -0800 2015</td>\n",
       "      <td>Thu Nov 19 11:31:55 -0800 2015</td>\n",
       "      <td>Wed Nov 18 00:00:00 -0800 2015</td>\n",
       "      <td>Tue Nov 17 00:00:00 -0800 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143082</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fcf6bca39e8f5333ba018b0e146ccfec</td>\n",
       "      <td>6837103</td>\n",
       "      <td>228c47ca18ed4c1598ae3c9214530b6b</td>\n",
       "      <td>5</td>\n",
       "      <td>Set in the late 1700s and early 1800s, the sto...</td>\n",
       "      <td>Tue Dec 06 13:19:29 -0800 2011</td>\n",
       "      <td>Mon Dec 19 11:28:19 -0800 2011</td>\n",
       "      <td>Mon Dec 19 00:00:00 -0800 2011</td>\n",
       "      <td>Tue Dec 06 00:00:00 -0800 2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101091</td>\n",
       "      <td>0.8885</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b8f6f163c2161555c6d887632b2ff4a2</td>\n",
       "      <td>17948485</td>\n",
       "      <td>6137b1fe0159b7eaa56a04293a00fd49</td>\n",
       "      <td>5</td>\n",
       "      <td>This book is the bomb! Love every single page ...</td>\n",
       "      <td>Fri Jul 26 17:43:19 -0700 2013</td>\n",
       "      <td>Sat Aug 10 20:06:19 -0700 2013</td>\n",
       "      <td>Sat Jul 27 00:00:00 -0700 2013</td>\n",
       "      <td>Fri Jul 26 00:00:00 -0700 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.203858</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33162c8e64b16bcbddc9808f3c716342</td>\n",
       "      <td>18405</td>\n",
       "      <td>c5b3dc0c0416d850380d80f5304be91f</td>\n",
       "      <td>5</td>\n",
       "      <td>Wherein I attempt to write a review using all ...</td>\n",
       "      <td>Wed Jun 30 08:01:44 -0700 2010</td>\n",
       "      <td>Tue Dec 31 06:07:21 -0800 2013</td>\n",
       "      <td>Fri Feb 18 00:00:00 -0800 2011</td>\n",
       "      <td>Sat Feb 12 00:00:00 -0800 2011</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>0.079210</td>\n",
       "      <td>0.9686</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37d8353490e210e2b3766336be99ebd4</td>\n",
       "      <td>26218626</td>\n",
       "      <td>234b51de9a79dfd51b5dc2b48df972ec</td>\n",
       "      <td>3</td>\n",
       "      <td>I can't believe I have only one volume left. T...</td>\n",
       "      <td>Wed Jun 29 19:48:42 -0700 2016</td>\n",
       "      <td>Thu Mar 30 07:18:31 -0700 2017</td>\n",
       "      <td>Thu Mar 30 07:18:31 -0700 2017</td>\n",
       "      <td>Thu Mar 30 00:00:00 -0700 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.058333</td>\n",
       "      <td>-0.5863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id   book_id  \\\n",
       "0  ed556b92506c3452b42fffed31697a1a  25125233   \n",
       "1  fcf6bca39e8f5333ba018b0e146ccfec   6837103   \n",
       "2  b8f6f163c2161555c6d887632b2ff4a2  17948485   \n",
       "3  33162c8e64b16bcbddc9808f3c716342     18405   \n",
       "4  37d8353490e210e2b3766336be99ebd4  26218626   \n",
       "\n",
       "                          review_id  rating  \\\n",
       "0  182718faad99666b70f73f3b7ffbbdb7       3   \n",
       "1  228c47ca18ed4c1598ae3c9214530b6b       5   \n",
       "2  6137b1fe0159b7eaa56a04293a00fd49       5   \n",
       "3  c5b3dc0c0416d850380d80f5304be91f       5   \n",
       "4  234b51de9a79dfd51b5dc2b48df972ec       3   \n",
       "\n",
       "                                         review_text  \\\n",
       "0  Jessica Broussard has only ever had her father...   \n",
       "1  Set in the late 1700s and early 1800s, the sto...   \n",
       "2  This book is the bomb! Love every single page ...   \n",
       "3  Wherein I attempt to write a review using all ...   \n",
       "4  I can't believe I have only one volume left. T...   \n",
       "\n",
       "                       date_added                    date_updated  \\\n",
       "0  Thu Nov 19 07:48:55 -0800 2015  Thu Nov 19 11:31:55 -0800 2015   \n",
       "1  Tue Dec 06 13:19:29 -0800 2011  Mon Dec 19 11:28:19 -0800 2011   \n",
       "2  Fri Jul 26 17:43:19 -0700 2013  Sat Aug 10 20:06:19 -0700 2013   \n",
       "3  Wed Jun 30 08:01:44 -0700 2010  Tue Dec 31 06:07:21 -0800 2013   \n",
       "4  Wed Jun 29 19:48:42 -0700 2016  Thu Mar 30 07:18:31 -0700 2017   \n",
       "\n",
       "                          read_at                      started_at  n_votes  \\\n",
       "0  Wed Nov 18 00:00:00 -0800 2015  Tue Nov 17 00:00:00 -0800 2015        0   \n",
       "1  Mon Dec 19 00:00:00 -0800 2011  Tue Dec 06 00:00:00 -0800 2011        0   \n",
       "2  Sat Jul 27 00:00:00 -0700 2013  Fri Jul 26 00:00:00 -0700 2013        0   \n",
       "3  Fri Feb 18 00:00:00 -0800 2011  Sat Feb 12 00:00:00 -0800 2011       46   \n",
       "4  Thu Mar 30 07:18:31 -0700 2017  Thu Mar 30 00:00:00 -0700 2017        0   \n",
       "\n",
       "   n_comments  review_text_textblob  review_text_nltk  review_text_sktlearn  \\\n",
       "0           0              0.143082            0.9964                   NaN   \n",
       "1           0              0.101091            0.8885                   4.0   \n",
       "2           0              0.203858            0.9961                   NaN   \n",
       "3          14              0.079210            0.9686                   4.0   \n",
       "4           0             -0.058333           -0.5863                   NaN   \n",
       "\n",
       "   review_text_keras  review_text_tf  \n",
       "0                  3               4  \n",
       "1                  5               5  \n",
       "2                  5               5  \n",
       "3                  5               5  \n",
       "4                  3               2  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we check out all the sentiment models into de dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3d4dc_row0_col0, #T_3d4dc_row1_col1, #T_3d4dc_row2_col2, #T_3d4dc_row3_col3, #T_3d4dc_row4_col4, #T_3d4dc_row5_col5, #T_3d4dc_row6_col6, #T_3d4dc_row7_col7, #T_3d4dc_row8_col8, #T_3d4dc_row9_col9, #T_3d4dc_row10_col10 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row0_col1, #T_3d4dc_row0_col3, #T_3d4dc_row0_col7, #T_3d4dc_row1_col0, #T_3d4dc_row2_col1, #T_3d4dc_row2_col3, #T_3d4dc_row2_col8, #T_3d4dc_row2_col9, #T_3d4dc_row2_col10, #T_3d4dc_row3_col0, #T_3d4dc_row5_col6, #T_3d4dc_row6_col4, #T_3d4dc_row6_col5, #T_3d4dc_row8_col0, #T_3d4dc_row8_col2, #T_3d4dc_row10_col0, #T_3d4dc_row10_col2 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row0_col2, #T_3d4dc_row5_col3, #T_3d4dc_row5_col10, #T_3d4dc_row6_col1 {\n",
       "  background-color: #4257c9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row0_col4, #T_3d4dc_row4_col3, #T_3d4dc_row4_col10 {\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row0_col5, #T_3d4dc_row0_col6, #T_3d4dc_row1_col3, #T_3d4dc_row1_col9, #T_3d4dc_row2_col6, #T_3d4dc_row5_col1, #T_3d4dc_row8_col1, #T_3d4dc_row10_col1 {\n",
       "  background-color: #465ecf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row0_col8, #T_3d4dc_row0_col10, #T_3d4dc_row2_col7, #T_3d4dc_row3_col2, #T_3d4dc_row4_col6, #T_3d4dc_row7_col0, #T_3d4dc_row9_col0, #T_3d4dc_row9_col2 {\n",
       "  background-color: #3c4ec2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row0_col9, #T_3d4dc_row1_col2, #T_3d4dc_row4_col0, #T_3d4dc_row5_col0, #T_3d4dc_row5_col2, #T_3d4dc_row6_col0 {\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row1_col4 {\n",
       "  background-color: #6180e9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row1_col5 {\n",
       "  background-color: #506bda;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row1_col6, #T_3d4dc_row3_col5, #T_3d4dc_row8_col5, #T_3d4dc_row9_col4, #T_3d4dc_row9_col5 {\n",
       "  background-color: #4c66d6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row1_col7, #T_3d4dc_row7_col5, #T_3d4dc_row10_col4 {\n",
       "  background-color: #4a63d3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row1_col8, #T_3d4dc_row4_col8, #T_3d4dc_row7_col4 {\n",
       "  background-color: #4961d2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row1_col10 {\n",
       "  background-color: #485fd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row2_col0, #T_3d4dc_row6_col2, #T_3d4dc_row7_col2 {\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row2_col4, #T_3d4dc_row5_col9 {\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row2_col5, #T_3d4dc_row3_col1, #T_3d4dc_row4_col9, #T_3d4dc_row5_col8, #T_3d4dc_row9_col1 {\n",
       "  background-color: #455cce;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row3_col4, #T_3d4dc_row7_col1, #T_3d4dc_row10_col5 {\n",
       "  background-color: #4b64d5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row3_col6 {\n",
       "  background-color: #a7c5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row3_col7, #T_3d4dc_row7_col3, #T_3d4dc_row7_col9 {\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row3_col8 {\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row3_col9 {\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row3_col10, #T_3d4dc_row10_col3 {\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row4_col1 {\n",
       "  background-color: #5977e3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row4_col2, #T_3d4dc_row5_col7 {\n",
       "  background-color: #3e51c5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row4_col5, #T_3d4dc_row5_col4 {\n",
       "  background-color: #f7b396;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row4_col7 {\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row6_col3 {\n",
       "  background-color: #9fbfff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row6_col7 {\n",
       "  background-color: #bed2f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row6_col8 {\n",
       "  background-color: #9ebeff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row6_col9 {\n",
       "  background-color: #a3c2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row6_col10 {\n",
       "  background-color: #a2c1ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row7_col6 {\n",
       "  background-color: #c5d6f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row7_col8 {\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row7_col10 {\n",
       "  background-color: #92b4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row8_col3 {\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row8_col4 {\n",
       "  background-color: #4f69d9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row8_col6 {\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row8_col7 {\n",
       "  background-color: #81a4fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row8_col9, #T_3d4dc_row9_col8 {\n",
       "  background-color: #ead4c8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row8_col10, #T_3d4dc_row10_col8 {\n",
       "  background-color: #e9d5cb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row9_col3 {\n",
       "  background-color: #f08a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row9_col6 {\n",
       "  background-color: #aac7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row9_col7, #T_3d4dc_row10_col7 {\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3d4dc_row9_col10, #T_3d4dc_row10_col9 {\n",
       "  background-color: #ee8669;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3d4dc_row10_col6 {\n",
       "  background-color: #a9c6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3d4dc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3d4dc_level0_col0\" class=\"col_heading level0 col0\" >user_id</th>\n",
       "      <th id=\"T_3d4dc_level0_col1\" class=\"col_heading level0 col1\" >book_id</th>\n",
       "      <th id=\"T_3d4dc_level0_col2\" class=\"col_heading level0 col2\" >review_id</th>\n",
       "      <th id=\"T_3d4dc_level0_col3\" class=\"col_heading level0 col3\" >rating</th>\n",
       "      <th id=\"T_3d4dc_level0_col4\" class=\"col_heading level0 col4\" >n_votes</th>\n",
       "      <th id=\"T_3d4dc_level0_col5\" class=\"col_heading level0 col5\" >n_comments</th>\n",
       "      <th id=\"T_3d4dc_level0_col6\" class=\"col_heading level0 col6\" >review_text_textblob</th>\n",
       "      <th id=\"T_3d4dc_level0_col7\" class=\"col_heading level0 col7\" >review_text_nltk</th>\n",
       "      <th id=\"T_3d4dc_level0_col8\" class=\"col_heading level0 col8\" >review_text_sktlearn</th>\n",
       "      <th id=\"T_3d4dc_level0_col9\" class=\"col_heading level0 col9\" >review_text_keras</th>\n",
       "      <th id=\"T_3d4dc_level0_col10\" class=\"col_heading level0 col10\" >review_text_tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3d4dc_level0_row0\" class=\"row_heading level0 row0\" >user_id</th>\n",
       "      <td id=\"T_3d4dc_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_3d4dc_row0_col1\" class=\"data row0 col1\" >-0.007307</td>\n",
       "      <td id=\"T_3d4dc_row0_col2\" class=\"data row0 col2\" >0.009213</td>\n",
       "      <td id=\"T_3d4dc_row0_col3\" class=\"data row0 col3\" >-0.008762</td>\n",
       "      <td id=\"T_3d4dc_row0_col4\" class=\"data row0 col4\" >-0.000984</td>\n",
       "      <td id=\"T_3d4dc_row0_col5\" class=\"data row0 col5\" >0.000615</td>\n",
       "      <td id=\"T_3d4dc_row0_col6\" class=\"data row0 col6\" >-0.000921</td>\n",
       "      <td id=\"T_3d4dc_row0_col7\" class=\"data row0 col7\" >-0.004950</td>\n",
       "      <td id=\"T_3d4dc_row0_col8\" class=\"data row0 col8\" >-0.009048</td>\n",
       "      <td id=\"T_3d4dc_row0_col9\" class=\"data row0 col9\" >-0.001957</td>\n",
       "      <td id=\"T_3d4dc_row0_col10\" class=\"data row0 col10\" >-0.007241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d4dc_level0_row1\" class=\"row_heading level0 row1\" >book_id</th>\n",
       "      <td id=\"T_3d4dc_row1_col0\" class=\"data row1 col0\" >-0.007307</td>\n",
       "      <td id=\"T_3d4dc_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_3d4dc_row1_col2\" class=\"data row1 col2\" >-0.006508</td>\n",
       "      <td id=\"T_3d4dc_row1_col3\" class=\"data row1 col3\" >0.031278</td>\n",
       "      <td id=\"T_3d4dc_row1_col4\" class=\"data row1 col4\" >0.093277</td>\n",
       "      <td id=\"T_3d4dc_row1_col5\" class=\"data row1 col5\" >0.034402</td>\n",
       "      <td id=\"T_3d4dc_row1_col6\" class=\"data row1 col6\" >0.018430</td>\n",
       "      <td id=\"T_3d4dc_row1_col7\" class=\"data row1 col7\" >0.049152</td>\n",
       "      <td id=\"T_3d4dc_row1_col8\" class=\"data row1 col8\" >0.033896</td>\n",
       "      <td id=\"T_3d4dc_row1_col9\" class=\"data row1 col9\" >0.028850</td>\n",
       "      <td id=\"T_3d4dc_row1_col10\" class=\"data row1 col10\" >0.032204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d4dc_level0_row2\" class=\"row_heading level0 row2\" >review_id</th>\n",
       "      <td id=\"T_3d4dc_row2_col0\" class=\"data row2 col0\" >0.009213</td>\n",
       "      <td id=\"T_3d4dc_row2_col1\" class=\"data row2 col1\" >-0.006508</td>\n",
       "      <td id=\"T_3d4dc_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_3d4dc_row2_col3\" class=\"data row2 col3\" >-0.008225</td>\n",
       "      <td id=\"T_3d4dc_row2_col4\" class=\"data row2 col4\" >-0.003161</td>\n",
       "      <td id=\"T_3d4dc_row2_col5\" class=\"data row2 col5\" >-0.005827</td>\n",
       "      <td id=\"T_3d4dc_row2_col6\" class=\"data row2 col6\" >0.000247</td>\n",
       "      <td id=\"T_3d4dc_row2_col7\" class=\"data row2 col7\" >0.001723</td>\n",
       "      <td id=\"T_3d4dc_row2_col8\" class=\"data row2 col8\" >-0.016075</td>\n",
       "      <td id=\"T_3d4dc_row2_col9\" class=\"data row2 col9\" >-0.012064</td>\n",
       "      <td id=\"T_3d4dc_row2_col10\" class=\"data row2 col10\" >-0.012693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d4dc_level0_row3\" class=\"row_heading level0 row3\" >rating</th>\n",
       "      <td id=\"T_3d4dc_row3_col0\" class=\"data row3 col0\" >-0.008762</td>\n",
       "      <td id=\"T_3d4dc_row3_col1\" class=\"data row3 col1\" >0.031278</td>\n",
       "      <td id=\"T_3d4dc_row3_col2\" class=\"data row3 col2\" >-0.008225</td>\n",
       "      <td id=\"T_3d4dc_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "      <td id=\"T_3d4dc_row3_col4\" class=\"data row3 col4\" >0.024718</td>\n",
       "      <td id=\"T_3d4dc_row3_col5\" class=\"data row3 col5\" >0.018426</td>\n",
       "      <td id=\"T_3d4dc_row3_col6\" class=\"data row3 col6\" >0.296344</td>\n",
       "      <td id=\"T_3d4dc_row3_col7\" class=\"data row3 col7\" >0.254273</td>\n",
       "      <td id=\"T_3d4dc_row3_col8\" class=\"data row3 col8\" >0.529924</td>\n",
       "      <td id=\"T_3d4dc_row3_col9\" class=\"data row3 col9\" >0.786581</td>\n",
       "      <td id=\"T_3d4dc_row3_col10\" class=\"data row3 col10\" >0.969597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d4dc_level0_row4\" class=\"row_heading level0 row4\" >n_votes</th>\n",
       "      <td id=\"T_3d4dc_row4_col0\" class=\"data row4 col0\" >-0.000984</td>\n",
       "      <td id=\"T_3d4dc_row4_col1\" class=\"data row4 col1\" >0.093277</td>\n",
       "      <td id=\"T_3d4dc_row4_col2\" class=\"data row4 col2\" >-0.003161</td>\n",
       "      <td id=\"T_3d4dc_row4_col3\" class=\"data row4 col3\" >0.024718</td>\n",
       "      <td id=\"T_3d4dc_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_3d4dc_row4_col5\" class=\"data row4 col5\" >0.669298</td>\n",
       "      <td id=\"T_3d4dc_row4_col6\" class=\"data row4 col6\" >-0.035070</td>\n",
       "      <td id=\"T_3d4dc_row4_col7\" class=\"data row4 col7\" >0.016577</td>\n",
       "      <td id=\"T_3d4dc_row4_col8\" class=\"data row4 col8\" >0.034834</td>\n",
       "      <td id=\"T_3d4dc_row4_col9\" class=\"data row4 col9\" >0.025871</td>\n",
       "      <td id=\"T_3d4dc_row4_col10\" class=\"data row4 col10\" >0.020630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d4dc_level0_row5\" class=\"row_heading level0 row5\" >n_comments</th>\n",
       "      <td id=\"T_3d4dc_row5_col0\" class=\"data row5 col0\" >0.000615</td>\n",
       "      <td id=\"T_3d4dc_row5_col1\" class=\"data row5 col1\" >0.034402</td>\n",
       "      <td id=\"T_3d4dc_row5_col2\" class=\"data row5 col2\" >-0.005827</td>\n",
       "      <td id=\"T_3d4dc_row5_col3\" class=\"data row5 col3\" >0.018426</td>\n",
       "      <td id=\"T_3d4dc_row5_col4\" class=\"data row5 col4\" >0.669298</td>\n",
       "      <td id=\"T_3d4dc_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "      <td id=\"T_3d4dc_row5_col6\" class=\"data row5 col6\" >-0.042743</td>\n",
       "      <td id=\"T_3d4dc_row5_col7\" class=\"data row5 col7\" >0.010405</td>\n",
       "      <td id=\"T_3d4dc_row5_col8\" class=\"data row5 col8\" >0.021635</td>\n",
       "      <td id=\"T_3d4dc_row5_col9\" class=\"data row5 col9\" >0.019113</td>\n",
       "      <td id=\"T_3d4dc_row5_col10\" class=\"data row5 col10\" >0.014689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d4dc_level0_row6\" class=\"row_heading level0 row6\" >review_text_textblob</th>\n",
       "      <td id=\"T_3d4dc_row6_col0\" class=\"data row6 col0\" >-0.000921</td>\n",
       "      <td id=\"T_3d4dc_row6_col1\" class=\"data row6 col1\" >0.018430</td>\n",
       "      <td id=\"T_3d4dc_row6_col2\" class=\"data row6 col2\" >0.000247</td>\n",
       "      <td id=\"T_3d4dc_row6_col3\" class=\"data row6 col3\" >0.296344</td>\n",
       "      <td id=\"T_3d4dc_row6_col4\" class=\"data row6 col4\" >-0.035070</td>\n",
       "      <td id=\"T_3d4dc_row6_col5\" class=\"data row6 col5\" >-0.042743</td>\n",
       "      <td id=\"T_3d4dc_row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "      <td id=\"T_3d4dc_row6_col7\" class=\"data row6 col7\" >0.390609</td>\n",
       "      <td id=\"T_3d4dc_row6_col8\" class=\"data row6 col8\" >0.287534</td>\n",
       "      <td id=\"T_3d4dc_row6_col9\" class=\"data row6 col9\" >0.306677</td>\n",
       "      <td id=\"T_3d4dc_row6_col10\" class=\"data row6 col10\" >0.301290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d4dc_level0_row7\" class=\"row_heading level0 row7\" >review_text_nltk</th>\n",
       "      <td id=\"T_3d4dc_row7_col0\" class=\"data row7 col0\" >-0.004950</td>\n",
       "      <td id=\"T_3d4dc_row7_col1\" class=\"data row7 col1\" >0.049152</td>\n",
       "      <td id=\"T_3d4dc_row7_col2\" class=\"data row7 col2\" >0.001723</td>\n",
       "      <td id=\"T_3d4dc_row7_col3\" class=\"data row7 col3\" >0.254273</td>\n",
       "      <td id=\"T_3d4dc_row7_col4\" class=\"data row7 col4\" >0.016577</td>\n",
       "      <td id=\"T_3d4dc_row7_col5\" class=\"data row7 col5\" >0.010405</td>\n",
       "      <td id=\"T_3d4dc_row7_col6\" class=\"data row7 col6\" >0.390609</td>\n",
       "      <td id=\"T_3d4dc_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "      <td id=\"T_3d4dc_row7_col8\" class=\"data row7 col8\" >0.213159</td>\n",
       "      <td id=\"T_3d4dc_row7_col9\" class=\"data row7 col9\" >0.252493</td>\n",
       "      <td id=\"T_3d4dc_row7_col10\" class=\"data row7 col10\" >0.254078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d4dc_level0_row8\" class=\"row_heading level0 row8\" >review_text_sktlearn</th>\n",
       "      <td id=\"T_3d4dc_row8_col0\" class=\"data row8 col0\" >-0.009048</td>\n",
       "      <td id=\"T_3d4dc_row8_col1\" class=\"data row8 col1\" >0.033896</td>\n",
       "      <td id=\"T_3d4dc_row8_col2\" class=\"data row8 col2\" >-0.016075</td>\n",
       "      <td id=\"T_3d4dc_row8_col3\" class=\"data row8 col3\" >0.529924</td>\n",
       "      <td id=\"T_3d4dc_row8_col4\" class=\"data row8 col4\" >0.034834</td>\n",
       "      <td id=\"T_3d4dc_row8_col5\" class=\"data row8 col5\" >0.021635</td>\n",
       "      <td id=\"T_3d4dc_row8_col6\" class=\"data row8 col6\" >0.287534</td>\n",
       "      <td id=\"T_3d4dc_row8_col7\" class=\"data row8 col7\" >0.213159</td>\n",
       "      <td id=\"T_3d4dc_row8_col8\" class=\"data row8 col8\" >1.000000</td>\n",
       "      <td id=\"T_3d4dc_row8_col9\" class=\"data row8 col9\" >0.550911</td>\n",
       "      <td id=\"T_3d4dc_row8_col10\" class=\"data row8 col10\" >0.541941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d4dc_level0_row9\" class=\"row_heading level0 row9\" >review_text_keras</th>\n",
       "      <td id=\"T_3d4dc_row9_col0\" class=\"data row9 col0\" >-0.001957</td>\n",
       "      <td id=\"T_3d4dc_row9_col1\" class=\"data row9 col1\" >0.028850</td>\n",
       "      <td id=\"T_3d4dc_row9_col2\" class=\"data row9 col2\" >-0.012064</td>\n",
       "      <td id=\"T_3d4dc_row9_col3\" class=\"data row9 col3\" >0.786581</td>\n",
       "      <td id=\"T_3d4dc_row9_col4\" class=\"data row9 col4\" >0.025871</td>\n",
       "      <td id=\"T_3d4dc_row9_col5\" class=\"data row9 col5\" >0.019113</td>\n",
       "      <td id=\"T_3d4dc_row9_col6\" class=\"data row9 col6\" >0.306677</td>\n",
       "      <td id=\"T_3d4dc_row9_col7\" class=\"data row9 col7\" >0.252493</td>\n",
       "      <td id=\"T_3d4dc_row9_col8\" class=\"data row9 col8\" >0.550911</td>\n",
       "      <td id=\"T_3d4dc_row9_col9\" class=\"data row9 col9\" >1.000000</td>\n",
       "      <td id=\"T_3d4dc_row9_col10\" class=\"data row9 col10\" >0.794006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d4dc_level0_row10\" class=\"row_heading level0 row10\" >review_text_tf</th>\n",
       "      <td id=\"T_3d4dc_row10_col0\" class=\"data row10 col0\" >-0.007241</td>\n",
       "      <td id=\"T_3d4dc_row10_col1\" class=\"data row10 col1\" >0.032204</td>\n",
       "      <td id=\"T_3d4dc_row10_col2\" class=\"data row10 col2\" >-0.012693</td>\n",
       "      <td id=\"T_3d4dc_row10_col3\" class=\"data row10 col3\" >0.969597</td>\n",
       "      <td id=\"T_3d4dc_row10_col4\" class=\"data row10 col4\" >0.020630</td>\n",
       "      <td id=\"T_3d4dc_row10_col5\" class=\"data row10 col5\" >0.014689</td>\n",
       "      <td id=\"T_3d4dc_row10_col6\" class=\"data row10 col6\" >0.301290</td>\n",
       "      <td id=\"T_3d4dc_row10_col7\" class=\"data row10 col7\" >0.254078</td>\n",
       "      <td id=\"T_3d4dc_row10_col8\" class=\"data row10 col8\" >0.541941</td>\n",
       "      <td id=\"T_3d4dc_row10_col9\" class=\"data row10 col9\" >0.794006</td>\n",
       "      <td id=\"T_3d4dc_row10_col10\" class=\"data row10 col10\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a4c916a5b0>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Matriz de correlaciones\n",
    "corrMatrix = df.corr() #Genereamos una matriz de correlaciones entre todas las variables\n",
    "corrMatrix.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 5 model of sentiment analysis, textblob and nltk are independent of rating, and sklearn, keras and tf are dependent of rating. The regression is high but we have to consider it because the 70% of the data was used for train. So it could be overfitting. For example for Keras the test data just gave us 50% de accuracy but if we use the whole data (train+test) it gives us almost 79%. The best results is by far given by Tensor flow with 97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18499, 16)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the string variables\n",
    "labelencoder = LabelEncoder()\n",
    "df['user_id'] = labelencoder.fit_transform(df['user_id'])\n",
    "df['book_id'] = labelencoder.fit_transform(df['book_id'])\n",
    "df['review_id'] = labelencoder.fit_transform(df['review_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target variable\n",
    "X = df.drop(['book_id', 'review_id', 'user_id',\"rating\",'date_added','date_updated','read_at','started_at', 'review_text'], axis=1)\n",
    "y = df[\"rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a linear regression model to the training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the star ratings on the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the predicted values to the nearest integer to obtain class labels\n",
    "y_pred_class = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9745864635125381\n"
     ]
    }
   ],
   "source": [
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred_class, average='weighted')\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a good F1 score.\n"
     ]
    }
   ],
   "source": [
    "# Check if the F1 score is over 70%\n",
    "if f1 > 0.7:\n",
    "    print(\"The model has a good F1 score.\")\n",
    "else:\n",
    "    print(\"The model needs improvement. Try using a different model or adding more features.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo 2 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a KNN model on the training data\n",
    "knn = KNeighborsRegressor(n_neighbors=50)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the ratings on the test data\n",
    "y_pred_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Model\n",
      "MSE:  0.14968594594594595\n",
      "R-squared:  0.8797989148822649\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MSE and R-squared for the KNN model\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "print(\"KNN Model\")\n",
    "print(\"MSE: \", mse_knn)\n",
    "print(\"R-squared: \", r2_knn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MSE, a smaller value indicates a better fit of the model to the data. A value of 0 means the model perfectly predicts the target. However, the MSE is sensitive to outliers, so it may not always be a reliable measure of the model's performance.\n",
    "\n",
    "For the R-squared, the value ranges from 0 to 1, with higher values indicating a better fit of the model to the data. A value of 1 means the model perfectly predicts the target. However, having a high R-squared does not always mean that the model is a good fit, as it can also increase with the addition of irrelevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation MSE:  0.18030843646045502\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation on the KNN model\n",
    "cv_scores_knn = cross_val_score(knn, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_knn = np.abs(cv_scores_knn)\n",
    "print(\"Cross-Validation MSE: \", cv_scores_knn.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=0)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Random Forest model on the training data\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "rf.fit(X_train, y_train) #it takes 9 minutes with the completed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the ratings on the test data\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model\n",
      "MSE:  0.07651076203110324\n",
      "R-squared:  0.9385601863875418\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MSE and R-squared for the Random Forest model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Model\")\n",
    "print(\"MSE: \", mse_rf)\n",
    "print(\"R-squared: \", r2_rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the random forest is slightly better performance but still far away from a satisfying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation MSE:  0.09378429179341084\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation on the Random Forest model\n",
    "cv_scores_rf = cross_val_score(rf, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_rf = np.abs(cv_scores_rf)\n",
    "print(\"Cross-Validation MSE: \", cv_scores_rf.mean()) ##it takes 35 minutes with the completed data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the Mean Squared Error (MSE) in cross-validation, the better the performance of the model. A good MSE score indicates that the model is making predictions that are close to the true values, while a high MSE score suggests that the model is making significant errors in its predictions.\n",
    "\n",
    "A MSE of 0 indicates a perfect fit, while a high MSE value suggests a poor fit. However, it is important to keep in mind that our range of working is between 1 to 5.\n",
    "\n",
    "In general, you can compare the cross-validation MSE of your model with the MSE of a baseline model or with the MSE of other models that you have tried to see if the performance of your model is good or bad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e9b2ac707fb302344000458e68859b5591fb383e6b1a3438783dffd073423d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
